# 代码清单

| \# | 代码内容 |
| --- | --- |
| [代码1-1](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_ClosedForm.html) | 查看`MountainCar-v0`的观测空间和动作空间 |
| [代码1-2](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_ClosedForm.html) | 根据指定确定性策略决定动作的智能体，用于`MountainCar-v0` |
| [代码1-3](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_ClosedForm.html) | 智能体和环境交互一个回合的代码 |
| [代码1-4](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_ClosedForm.html) | 运行100回合求平均以测试性能 |
| [代码1-5](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCarContinuous-v0_ClosedForm.html) | 查看`MountainCarContinuous-v0`的观测空间和动作空间 |
| [代码1-6](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCarContinuous-v0_ClosedForm.html) | 用于求解`MountainCarContinous-v0`的智能体 |
| [代码2-1](https://zhiqingxiao.github.io/rl-book/en2023/code/HungryFull_demo.html) | 求解示例Bellman期望方程 |
| [代码2-2](https://zhiqingxiao.github.io/rl-book/en2023/code/HungryFull_demo.html) | 求解示例Bellman最优方程 |
| [代码2-3](https://zhiqingxiao.github.io/rl-book/en2023/code/CliffWalking-v0_Bellman_demo.html) | 导入`CliffWalking-v0`环境并查看环境信息 |
| [代码2-4](https://zhiqingxiao.github.io/rl-book/en2023/code/CliffWalking-v0_Bellman_demo.html) | 用Bellman方程求解状态价值和动作价值 |
| [代码2-5](https://zhiqingxiao.github.io/rl-book/en2023/code/CliffWalking-v0_Bellman_demo.html) | 用线性规划求解最优价值 |
| [代码2-6](https://zhiqingxiao.github.io/rl-book/en2023/code/CliffWalking-v0_Bellman_demo.html) | 用最优动作价值确定最优确定性策略 |
| [代码3-1](https://zhiqingxiao.github.io/rl-book/en2023/code/FrozenLake-v1_DP_demo.html) | 导入`FrozenLake-v1`并查看基本信息 |
| [代码3-2](https://zhiqingxiao.github.io/rl-book/en2023/code/FrozenLake-v1_DP_demo.html) | 用策略执行一个回合 |
| [代码3-3](https://zhiqingxiao.github.io/rl-book/en2023/code/FrozenLake-v1_DP_demo.html) | 统计随机策略的回合奖励 |
| [代码3-4](https://zhiqingxiao.github.io/rl-book/en2023/code/FrozenLake-v1_DP_demo.html) | 策略评估的实现 |
| [代码3-5](https://zhiqingxiao.github.io/rl-book/en2023/code/FrozenLake-v1_DP_demo.html) | 对随机策略进行策略评估 |
| [代码3-6](https://zhiqingxiao.github.io/rl-book/en2023/code/FrozenLake-v1_DP_demo.html) | 策略改进的实现 |
| [代码3-7](https://zhiqingxiao.github.io/rl-book/en2023/code/FrozenLake-v1_DP_demo.html) | 对随机策略进行策略改进 |
| [代码3-8](https://zhiqingxiao.github.io/rl-book/en2023/code/FrozenLake-v1_DP_demo.html) | 策略迭代的实现 |
| [代码3-9](https://zhiqingxiao.github.io/rl-book/en2023/code/FrozenLake-v1_DP_demo.html) | 利用策略迭代求解最优策略并测试 |
| [代码3-10](https://zhiqingxiao.github.io/rl-book/en2023/code/FrozenLake-v1_DP_demo.html) | 价值迭代的实现 |
| [代码3-11](https://zhiqingxiao.github.io/rl-book/en2023/code/FrozenLake-v1_DP_demo.html) | 用价值迭代算法求解最优策略 |
| [代码4-1](https://zhiqingxiao.github.io/rl-book/en2023/code/Blackjack-v1_MonteCarlo_demo.html) | 玩一个回合 |
| [代码4-2](https://zhiqingxiao.github.io/rl-book/en2023/code/Blackjack-v1_MonteCarlo_demo.html) | 同策回合更新策略评估 |
| [代码4-3](https://zhiqingxiao.github.io/rl-book/en2023/code/Blackjack-v1_MonteCarlo_demo.html) | 绘制以状态为指标的3维数组 |
| [代码4-4](https://zhiqingxiao.github.io/rl-book/en2023/code/Blackjack-v1_MonteCarlo_demo.html) | 带起始探索的同策回合更新 |
| [代码4-5](https://zhiqingxiao.github.io/rl-book/en2023/code/Blackjack-v1_MonteCarlo_demo.html) | 基于柔性策略的同策回合更新 |
| [代码4-6](https://zhiqingxiao.github.io/rl-book/en2023/code/Blackjack-v1_MonteCarlo_demo.html) | 重要性采样策略评估 |
| [代码4-7](https://zhiqingxiao.github.io/rl-book/en2023/code/Blackjack-v1_MonteCarlo_demo.html) | 柔性策略重要性采样最优策略求解 |
| [代码5-1](https://zhiqingxiao.github.io/rl-book/en2023/code/Taxi-v3_SARSA_demo.html) | 初始化环境并可视化 |
| [代码5-2](https://zhiqingxiao.github.io/rl-book/en2023/code/Taxi-v3_SARSA_demo.html) | SARSA算法智能体的实现 |
| [代码5-3](https://zhiqingxiao.github.io/rl-book/en2023/code/Taxi-v3_SARSA_demo.html) | 训练智能体 |
| [代码5-4](https://zhiqingxiao.github.io/rl-book/en2023/code/Taxi-v3_ExpectedSARSA.html) | 期望SARSA算法智能体 |
| [代码5-5](https://zhiqingxiao.github.io/rl-book/en2023/code/Taxi-v3_QLearning.html) | Q学习智能体 |
| [代码5-6](https://zhiqingxiao.github.io/rl-book/en2023/code/Taxi-v3_DoubleQLearning.html) | 双重Q学习智能体 |
| [代码5-7](https://zhiqingxiao.github.io/rl-book/en2023/code/Taxi-v3_SARSALambda.html) | SARSA $(\lambda)$ 算法智能体 |
| [代码6-1](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_SARSA_demo.html) | 导入小车上山环境 |
| [代码6-2](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_SARSA_demo.html) | 总是向右施力的智能体 |
| [代码6-3](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_SARSA_demo.html) | 砖瓦编码 |
| [代码6-4](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_SARSA_demo.html) | 函数近似SARSA算法智能体 |
| [代码6-5](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_SARSAlambda.html)| 函数近似SARSA $(\lambda)$ 智能体 |
| [代码6-6](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_DQN_tf.html) | 经验回放的实现 |
| [代码6-7](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_DQN_tf.html) | 带目标网络的深度Q网络智能体（TensorFlow版本） |
| [代码6-8](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_DQN_torch.html) | 带目标网络的深度Q网络智能体（PyTorch版本） |
| [代码6-9](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_DoubleDQN_tf.html) | 双重深度Q网络智能体（TensorFlow版本） |
| [代码6-10](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_DoubleDQN_torch.html) | 双重深度Q网络智能体（PyTorch版本） |
| [代码6-11](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_DuelDQN_tf.html) | 决斗网络（TensorFlow版本） |
| [代码6-12](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_DuelDQN_torch.html) | 决斗网络（PyTorch版本） |
| [代码6-13](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_DuelDQN_tf.html) | 决斗Q网络智能体（TensorFlow 版本） |
| [代码6-14](https://zhiqingxiao.github.io/rl-book/en2023/code/MountainCar-v0_DuelDQN_torch.html) | 决斗Q网络智能体（PyTorch版本） |
| [代码7-1](https://zhiqingxiao.github.io/rl-book/en2023/code/CartPole-v0_VPG_tf.html) | 同策策略梯度算法智能体（TensorFlow版本） |
| [代码7-2](https://zhiqingxiao.github.io/rl-book/en2023/code/CartPole-v0_VPG_torch.html) | 同策策略梯度算法智能体（PyTorch版本） |
| [代码7-3](https://zhiqingxiao.github.io/rl-book/en2023/code/CartPole-v0_VPGwBaseline_tf.html) | 带基线的同策策略梯度算法智能体（TensorFlow版本） |
| [代码7-4](https://zhiqingxiao.github.io/rl-book/en2023/code/CartPole-v0_VPGwBaseline_torch.html) | 带基线的同策策略梯度算法智能体（PyTorch版本） |
| [代码7-5](https://zhiqingxiao.github.io/rl-book/en2023/code/CartPole-v0_OffPolicyVPG_tf.html) | 异策策略梯度算法智能体（TensorFlow版本） |
| [代码7-6](https://zhiqingxiao.github.io/rl-book/en2023/code/CartPole-v0_OffPolicyVPG_torch.html) | 异策策略梯度算法智能体（PyTorch版本） |
| [代码7-7](https://zhiqingxiao.github.io/rl-book/en2023/code/CartPole-v0_OffPolicyVPGwBaseline_tf.html) | 带基线的异策策略梯度算法智能体（TensorFlow版本） |
| [代码7-8](https://zhiqingxiao.github.io/rl-book/en2023/code/CartPole-v0_OffPolicyVPGwBaseline_torch.html) | 带基线的异策策略梯度算法智能体（PyTorch版本） |
| [代码8-1](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_QActorCritic_tf.html) | 动作价值执行者/评论者算法（TensorFlow版本） |
| [代码8-2](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_QActorCritic_torch.html) | 动作价值执行者/评论者算法（PyTorch版本） |
| [代码8-3](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_AdvantageActorCritic_tf.html) | 优势执行者/评论者算法的智能体实现（TensorFlow版本） |
| [代码8-4](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_AdvantageActorCritic_torch.html) | 优势执行者/评论者算法的智能体实现（PyTorch版本） |
| [代码8-5](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_EligibilityTraceAC_tf.html) | 带资格迹的执行者/评论者（TensorFlow版本） |
| [代码8-6](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_EligibilityTraceAC_torch.html) | 带资格迹的执行者/评论者（PyTorch版本） |
| [代码8-7](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_PPO_tf.html) | 邻近策略优化的经验回放类 |
| [代码8-8](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_PPO_tf.html) | 邻近策略优化算法智能体（TensorFlow版本） |
| [代码8-9](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_PPO_torch.html) | 邻近策略优化算法智能体（PyTorch版本） |
| [代码8-10](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_NPG_tf.html) | 计算共轭梯度（TensorFlow版本） |
| [代码8-11](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_NPG_torch.html) | 计算共轭梯度（PyTorch版本） |
| [代码8-12](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_NPG_tf.html) | 自然策略梯度算法智能体（TensorFlow版本） |
| [代码8-13](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_NPG_torch.html) | 自然策略梯度算法智能体（PyTorch版本） |
| [代码8-14](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_TRPO_tf.html) | 信赖域策略优化算法智能体（TensorFlow版本） |
| [代码8-15](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_TRPO_torch.html) | 信赖域策略优化算法智能体（PyTorch版本） |
| [代码8-16](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_OffPAC_tf.html) | 异策执行者/评论者算法智能体（TensorFlow版本） |
| [代码8-17](https://zhiqingxiao.github.io/rl-book/en2023/code/Acrobot-v1_OffPAC_torch.html) | 异策执行者/评论者算法智能体（PyTorch版本） |
| [代码9-1](https://zhiqingxiao.github.io/rl-book/en2023/code/Pendulum-v1_DDPG_tf.html) | OU过程 |
| [代码9-2](https://zhiqingxiao.github.io/rl-book/en2023/code/Pendulum-v1_DDPG_tf.html) | 深度确定性策略梯度算法的智能体（TensorFlow版本） |
| [代码9-3](https://zhiqingxiao.github.io/rl-book/en2023/code/Pendulum-v1_DDPG_torch.html) | 深度确定性策略梯度算法的智能体（PyTorch版本） |
| [代码9-4](https://zhiqingxiao.github.io/rl-book/en2023/code/Pendulum-v1_TD3_tf.html) | 双重延迟深度确定性算法智能体（TensorFlow版） |
| [代码9-5](https://zhiqingxiao.github.io/rl-book/en2023/code/Pendulum-v1_TD3_torch.html) | 双重延迟深度确定性算法智能体（PyTorch版） |
| [代码10-1](https://zhiqingxiao.github.io/rl-book/en2023/code/LunarLander-v2_ClosedForm.html) | `LunarLander-v2`的闭式解 |
| [代码10-2](https://zhiqingxiao.github.io/rl-book/en2023/code/LunarLanderContinuous-v2_ClosedForm.html) | `LunarLanderContinuous-v2`的闭式解 |
| [代码10-3](https://zhiqingxiao.github.io/rl-book/en2023/code/LunarLander-v2_SQL_tf.html) | 柔性Q学习智能体（使用TensorFlow） |
| [代码10-4](https://zhiqingxiao.github.io/rl-book/en2023/code/LunarLander-v2_SQL_torch.html) | 柔性Q学习智能体（使用PyTorch） |
| [代码10-5](https://zhiqingxiao.github.io/rl-book/en2023/code/LunarLander-v2_SACwoA_tf.html) | 柔性执行者/评论者算法智能体（TensorFlow版） |
| [代码10-6](https://zhiqingxiao.github.io/rl-book/en2023/code/LunarLander-v2_SACwoA_torch.html) | 柔性执行者/评论者算法智能体（PyTorch版） |
| [代码10-7](https://zhiqingxiao.github.io/rl-book/en2023/code/LunarLander-v2_SACwA_tf.html) | 带自动熵调节的柔性执行者/评论者算法智能体（TensorFlow版） |
| [代码10-8](https://zhiqingxiao.github.io/rl-book/en2023/code/LunarLander-v2_SACwA_torch.html) | 带自动熵调节的柔性执行者/评论者算法智能体（PyTorch版） |
| [代码10-9](https://zhiqingxiao.github.io/rl-book/en2023/code/LunarLanderContinuous-v2_SACwA_tf.html) | 用于连续动作空间的带自动熵调节的柔性执行者/评论者算法（使用TensorFlow） |
| [代码10-10](https://zhiqingxiao.github.io/rl-book/en2023/code/LunarLanderContinuous-v2_SACwA_torch.html) | 用于连续动作空间的带自动熵调节的柔性执行者/评论者算法（使用PyTorch） |
| [代码11-1](https://zhiqingxiao.github.io/rl-book/en2023/code/BipedalWalker-v3_ClosedForm.html) | `BipedalWalker-v3`的闭式解 |
| [代码11-2](https://zhiqingxiao.github.io/rl-book/en2023/code/BipedalWalker-v3_ES.html) | 进化算法智能体 |
| [代码11-3](https://zhiqingxiao.github.io/rl-book/en2023/code/BipedalWalker-v3_ES.html) | 训练和测试进化算法智能体 |
| [代码11-4](https://zhiqingxiao.github.io/rl-book/en2023/code/BipedalWalker-v3_ARS.html) | 增强随机搜索算法智能体 |
| [代码12-1](https://zhiqingxiao.github.io/rl-book/en2023/code/PongNoFrameskip-v4_ClosedForm.html) | `PongNoFrameskip-v4`的闭式解 |
| [代码12-2](https://zhiqingxiao.github.io/rl-book/en2023/code/PongNoFrameskip-v4_CategoricalDQN_tf.html) | 包装后的环境类 |
| [代码12-3](https://zhiqingxiao.github.io/rl-book/en2023/code/PongNoFrameskip-v4_CategoricalDQN_tf.html) | 类别深度Q网络算法智能体（TensorFlow版本） |
| [代码12-4](https://zhiqingxiao.github.io/rl-book/en2023/code/PongNoFrameskip-v4_CategoricalDQN_torch.html) | 类别深度Q网络算法智能体（PyTorch版本） |
| [代码12-5](https://zhiqingxiao.github.io/rl-book/en2023/code/PongNoFrameskip-v4_QRDQN_tf.html) | 分位数回归深度Q网络算法智能体（TensorFlow版本） |
| [代码12-6](https://zhiqingxiao.github.io/rl-book/en2023/code/PongNoFrameskip-v4_QRDQN_torch.html) | 分位数回归深度Q网络算法智能体（PyTorch版本） |
| [代码12-7](https://zhiqingxiao.github.io/rl-book/en2023/code/PongNoFrameskip-v4_IQN_tf.html) | 分位网络（TensorFlow版本） |
| [代码12-8](https://zhiqingxiao.github.io/rl-book/en2023/code/PongNoFrameskip-v4_IQN_torch.html) | 分位网络（PyTorch版本） |
| [代码12-9](https://zhiqingxiao.github.io/rl-book/en2023/code/PongNoFrameskip-v4_IQN_tf.html) | 含蓄分位网络智能体（TensorFlow版本） |
| [代码12-10](https://zhiqingxiao.github.io/rl-book/en2023/code/PongNoFrameskip-v4_IQN_torch.html) | 含蓄分位网络智能体（PyTorch版本） |
| [代码13-1](https://zhiqingxiao.github.io/rl-book/en2023/code/BernoulliMABEnv-v0_demo.html) | 环境类`BernoulliMABEnv` |
| [代码13-2](https://zhiqingxiao.github.io/rl-book/en2023/code/BernoulliMABEnv-v0_demo.html) | 将环境类`BernoulliMABEnv`注册到Gym库里 |
| [代码13-3](https://zhiqingxiao.github.io/rl-book/en2023/code/BernoulliMABEnv-v0_demo.html) | 用 $\epsilon$ 贪心策略求解 |
| [代码13-4](https://zhiqingxiao.github.io/rl-book/en2023/code/BernoulliMABEnv-v0_demo.html) | 估计平均遗憾 |
| [代码13-5](https://zhiqingxiao.github.io/rl-book/en2023/code/BernoulliMABEnv-v0_demo.html) | 用第一置信上界求解 |
| [代码13-6](https://zhiqingxiao.github.io/rl-book/en2023/code/BernoulliMABEnv-v0_demo.html) | 用Bayesian置信上界求解 |
| [代码13-7](https://zhiqingxiao.github.io/rl-book/en2023/code/BernoulliMABEnv-v0_demo.html) | 用Thompson采样求解 |
| [代码14-1](https://github.com/ZhiqingXiao/boardgame2/blob/master/boardgame2/env.py) |`BoardGameEnv`类的构造函数 |
| [代码14-2](https://github.com/ZhiqingXiao/boardgame2/blob/master/boardgame2/env.py) | `BoardGameEnv`类的`is_vaild()`函数、`has_valid()`函数和`get_valid()`函数 |
| [代码14-3](https://github.com/ZhiqingXiao/boardgame2/blob/master/boardgame2/kinarow.py) | `KInARowEnv`类的`get_winner()`函数 |
| [代码14-4](https://github.com/ZhiqingXiao/boardgame2/blob/master/boardgame2/env.py) | `BoardGameEnv`类的`next_step()`函数及其辅助函数`get_next_state()` |
| [代码14-5](https://github.com/ZhiqingXiao/boardgame2/blob/master/boardgame2/env.py) | `BoardGameEnv`类的`next_step()`函数及其辅助函数`get_next_state()` |
| [代码14-6](https://zhiqingxiao.github.io/rl-book/en2023/code/TicTacToe-v0_ExhaustiveSearch.html) | 穷尽式搜索 |
| [代码14-7](https://zhiqingxiao.github.io/rl-book/en2023/code/TicTacToe-v0_ExhaustiveSearch.html) | 自我对弈 |
| [代码14-8](https://zhiqingxiao.github.io/rl-book/en2023/code/TicTacToe-v0_AlphaZero_tf.html) | AlphaZero智能体经验回放 |
| [代码14-9](https://zhiqingxiao.github.io/rl-book/en2023/code/TicTacToe-v0_AlphaZero_tf.html) | AlphaZero用的网络（TensorFlow版本） |
| [代码14-10](https://zhiqingxiao.github.io/rl-book/en2023/code/TicTacToe-v0_AlphaZero_torch.html) | AlphaZero用的网络（PyTorch版本） |
| [代码14-11](https://zhiqingxiao.github.io/rl-book/en2023/code/TicTacToe-v0_AlphaZero_tf.html) | AlphaZero智能体（TensorFlow版本） |
| [代码14-12](https://zhiqingxiao.github.io/rl-book/en2023/code/TicTacToe-v0_AlphaZero_torch.html) | AlphaZero智能体（PyTorch版本） |
| [代码15-1](https://zhiqingxiao.github.io/rl-book/en2023/code/Tiger-v0_ClosedForm.html) | 任务“老虎”环境类`TigerEnv` |
| [代码15-2](https://zhiqingxiao.github.io/rl-book/en2023/code/Tiger-v0_ClosedForm.html) | 注册环境类`TigerEnv` |
| [代码15-3](https://zhiqingxiao.github.io/rl-book/en2023/code/Tiger-v0_ClosedForm.html) | 折扣因子 $\gamma=1$ 时的最优策略 |
| [代码15-4](https://zhiqingxiao.github.io/rl-book/en2023/code/Tiger-v0_Plan_demo.html) | 信念价值迭代 |
| [代码15-5](https://zhiqingxiao.github.io/rl-book/en2023/code/Tiger-v0_Plan_demo.html) | 用基于点的价值迭代求解 |
| [代码16-1](https://zhiqingxiao.github.io/rl-book/en2023/code/HumanoidBulletEnv-v0_ClosedForm_demo.html) | 调整摄像头 |
| [代码16-2](https://zhiqingxiao.github.io/rl-book/en2023/code/HumanoidBulletEnv-v0_ClosedForm_demo.html) | 与环境交互并渲染 |
| [代码16-3](https://zhiqingxiao.github.io/rl-book/en2023/code/HumanoidBulletEnv-v0_BC_tf.html) | 状态动作对的经验回放 |
| [代码16-4](https://zhiqingxiao.github.io/rl-book/en2023/code/HumanoidBulletEnv-v0_BC_tf.html) | 行为克隆模仿学习智能体（TensorFlow版本） |
| [代码16-5](https://zhiqingxiao.github.io/rl-book/en2023/code/HumanoidBulletEnv-v0_BC_torch.html) | 行为克隆模仿学习智能体（PyTorch版本） |
| [代码16-6](https://zhiqingxiao.github.io/rl-book/en2023/code/HumanoidBulletEnv-v0_GAILPPO_tf.html) | 生成对抗模仿学习邻近策略优化算法智能体（TensorFlow版本） |
| [代码16-7](https://zhiqingxiao.github.io/rl-book/en2023/code/HumanoidBulletEnv-v0_GAILPPO_torch.html) | 生成对抗模仿学习邻近策略优化算法智能体（PyTorch版本） |
