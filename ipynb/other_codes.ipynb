{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "01234567890123456789012345678901234567890123456789012345678901234567890123456789\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331505b6",
   "metadata": {},
   "source": [
    "Chapter 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11886d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0', new_step_api=True)\n",
    "\n",
    "print([env_spec.id for env_spec in gym.envs.registry.all()])\n",
    "\n",
    "env.reset()\n",
    "\n",
    "env.render()\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.spec.reward_threshold\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0', new_step_api=True)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e07abb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:19:10 [INFO] entry_point: gym.envs.classic_control:MountainCarEnv\n",
      "16:19:10 [INFO] reward_threshold: -110.0\n",
      "16:19:10 [INFO] nondeterministic: False\n",
      "16:19:10 [INFO] max_episode_steps: 200\n",
      "16:19:10 [INFO] order_enforce: True\n",
      "16:19:10 [INFO] kwargs: {}\n",
      "16:19:10 [INFO] namespace: None\n",
      "16:19:10 [INFO] name: MountainCar\n",
      "16:19:10 [INFO] version: 0\n",
      "16:19:10 [INFO] min_position: -1.2\n",
      "16:19:10 [INFO] max_position: 0.6\n",
      "16:19:10 [INFO] max_speed: 0.07\n",
      "16:19:10 [INFO] goal_position: 0.5\n",
      "16:19:10 [INFO] goal_velocity: 0\n",
      "16:19:10 [INFO] force: 0.001\n",
      "16:19:10 [INFO] gravity: 0.0025\n",
      "16:19:10 [INFO] low: [-1.2  -0.07]\n",
      "16:19:10 [INFO] high: [0.6  0.07]\n",
      "16:19:10 [INFO] screen: None\n",
      "16:19:10 [INFO] clock: None\n",
      "16:19:10 [INFO] isopen: True\n",
      "16:19:10 [INFO] action_space: Discrete(3)\n",
      "16:19:10 [INFO] observation_space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "16:19:10 [INFO] spec: EnvSpec(entry_point='gym.envs.classic_control:MountainCarEnv', reward_threshold=-110.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, kwargs={}, namespace=None, name='MountainCar', version=0)\n",
      "16:19:10 [INFO] _np_random: RandomNumberGenerator(PCG64)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import gym\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        stream=sys.stdout, datefmt='%H:%M:%S')\n",
    "\n",
    "\n",
    "import gym\n",
    "env = gym.make('MountainCar-v0', new_step_api=True)\n",
    "for key in vars(env.spec):\n",
    "    logging.info('%s: %s', key, vars(env.spec)[key])\n",
    "for key in vars(env.unwrapped):\n",
    "    logging.info('%s: %s', key, vars(env.unwrapped)[key])\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddb8f322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:21:16 [INFO] entry_point: gym.envs.classic_control:Continuous_MountainCarEnv\n",
      "16:21:16 [INFO] reward_threshold: 90.0\n",
      "16:21:16 [INFO] nondeterministic: False\n",
      "16:21:16 [INFO] max_episode_steps: 999\n",
      "16:21:16 [INFO] order_enforce: True\n",
      "16:21:16 [INFO] kwargs: {}\n",
      "16:21:16 [INFO] namespace: None\n",
      "16:21:16 [INFO] name: MountainCarContinuous\n",
      "16:21:16 [INFO] version: 0\n",
      "16:21:16 [INFO] min_action: -1.0\n",
      "16:21:16 [INFO] max_action: 1.0\n",
      "16:21:16 [INFO] min_position: -1.2\n",
      "16:21:16 [INFO] max_position: 0.6\n",
      "16:21:16 [INFO] max_speed: 0.07\n",
      "16:21:16 [INFO] goal_position: 0.45\n",
      "16:21:16 [INFO] goal_velocity: 0\n",
      "16:21:16 [INFO] power: 0.0015\n",
      "16:21:16 [INFO] low_state: [-1.2  -0.07]\n",
      "16:21:16 [INFO] high_state: [0.6  0.07]\n",
      "16:21:16 [INFO] screen: None\n",
      "16:21:16 [INFO] clock: None\n",
      "16:21:16 [INFO] isopen: True\n",
      "16:21:16 [INFO] action_space: Box(-1.0, 1.0, (1,), float32)\n",
      "16:21:16 [INFO] observation_space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "16:21:16 [INFO] spec: EnvSpec(entry_point='gym.envs.classic_control:Continuous_MountainCarEnv', reward_threshold=90.0, nondeterministic=False, max_episode_steps=999, order_enforce=True, kwargs={}, namespace=None, name='MountainCarContinuous', version=0)\n",
      "16:21:16 [INFO] _np_random: RandomNumberGenerator(PCG64)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "import gym\n",
    "env = gym.make('MountainCarContinuous-v0', new_step_api=True)\n",
    "env.seed(0)\n",
    "for key in vars(env.spec):\n",
    "    logging.info('%s: %s', key, vars(env.spec)[key])\n",
    "for key in vars(env.unwrapped):\n",
    "    logging.info('%s: %s', key, vars(env.unwrapped)[key])\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f42081",
   "metadata": {},
   "source": [
    "Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd90cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "policy = np.ones((env.nS, env.nA)) / env.nA\n",
    "\n",
    "actions = np.ones(env.nS, dtype=int)\n",
    "actions[36] = 0\n",
    "actions[11::12] = 2\n",
    "policy = np.eye(env.nA)[actions]\n",
    "\n",
    "policy = np.random.uniform(size=(env.nS, env.nA))\n",
    "policy = policy / np.sum(policy, axis=1, keepdims=True)\n",
    "\n",
    "state_values, action_values = evaluate_bellman(env, policy)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c833b77",
   "metadata": {},
   "source": [
    "Chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "import gym\n",
    "env = gym.make('FrozenLake-v1', new_step_api=True)\n",
    "logging.info('observation space = %s', env.observation_space)\n",
    "logging.info('action space = %s', env.action_space)\n",
    "logging.info('number of states = %s', env.observation_space.n)\n",
    "logging.info('number of actions = %s', env.action_space.n)\n",
    "logging.info('reward threshold = %s', env.spec.reward_threshold)\n",
    "logging.info('max episode steps = %s', env.spec.max_episode_steps)\n",
    "\n",
    "logging.info('P[14] = %s', env.unwrapped.P[14])\n",
    "logging.info('P[14][2] = %s', env.unwrapped.P[14][2])\n",
    "env.unwrapped.P[14][2]\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b649ed9",
   "metadata": {},
   "source": [
    "Chapter 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5417f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "policy = np.zeros((22, 11, 2, 2))\n",
    "policy[20:, :, :, 0] = 1 # stand when >=20\n",
    "policy[:20, :, :, 1] = 1 # hit when <20\n",
    "\n",
    "q = evaluate_action_monte_carlo(env, policy) # action value\n",
    "v = (q * policy).sum(axis=-1) # state value\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1a644f",
   "metadata": {},
   "source": [
    "Chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ccad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "pd.DataFrame(agent.q)\n",
    "\n",
    "policy = np.eye(agent.action_n)[agent.q.argmax(axis=-1)] \n",
    "pd.DataFrame(policy)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba2bd3f",
   "metadata": {},
   "source": [
    "Chapter 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0da014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "Box(low=0, high=255, shape=(屏幕长,屏幕宽,3), dtype=np.uint8) # 不带ram的版本\n",
    "Box(low=0, high=255, shape=(128,), dtype=np.uint8) # 带ram的版本\n",
    "\n",
    "Box(low=0, high=255, shape=(length,width,3), dtype=np.uint8) # without ram\n",
    "Box(low=0, high=255, shape=(128,), dtype=np.uint8) # with ram\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15070ff3",
   "metadata": {},
   "source": [
    "Chapter 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1165ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "def is_valid(self, state, action):\n",
    "    \"\"\"\n",
    "    Check whether the action is valid for current state.\n",
    "\n",
    "    Parameters\n",
    "    ----\n",
    "    state : (np.array, int)    board and player\n",
    "    action : np.array   location and skip\n",
    "\n",
    "    Returns\n",
    "    ----\n",
    "    valid : bool\n",
    "    \"\"\"\n",
    "    board, _ = state\n",
    "    if not is_index(board, action):\n",
    "        return False\n",
    "    x, y = action\n",
    "    return board[x, y] == EMPTY\n",
    "\n",
    "def get_valid(self, state):\n",
    "    \"\"\"\n",
    "    Get all valid locations for the current state.\n",
    "\n",
    "    Parameters\n",
    "    ----\n",
    "    state : (np.array, int)    board and player\n",
    "\n",
    "    Returns\n",
    "    ----\n",
    "    valid : np.array     current valid place for the player\n",
    "    \"\"\"\n",
    "    board, _ = state\n",
    "    valid = np.zeros_like(board, dtype=np.int8)\n",
    "    for x in range(board.shape[0]):\n",
    "        for y in range(board.shape[1]):\n",
    "            valid[x, y] = self.is_valid(state, np.array([x, y]))\n",
    "    return valid\n",
    "\n",
    "def has_valid(self, state):\n",
    "    \"\"\"\n",
    "    Check whether there are valid locations for current state.\n",
    "\n",
    "    Parameters\n",
    "    ----\n",
    "    state : (np.array, int)    board and player\n",
    "\n",
    "    Returns\n",
    "    ----\n",
    "    has_valid : bool\n",
    "    \"\"\"\n",
    "    board = state[0]\n",
    "    for x in range(board.shape[0]):\n",
    "        for y in range(board.shape[1]):\n",
    "            if self.is_valid(state, np.array([x, y])):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b01c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "def get_winner(self, state):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----\n",
    "    state : (np.array, int)   board and player. player info is not used\n",
    "\n",
    "    Returns\n",
    "    ----\n",
    "    winner : None or int\n",
    "        - None   if the game is not ended and the winner is not determined\n",
    "        - int    the winner\n",
    "    \"\"\"\n",
    "    board, _ = state\n",
    "    for player in [BLACK, WHITE]:\n",
    "        for x in range(board.shape[0]):\n",
    "            for y in range(board.shape[1]):\n",
    "                for dx, dy in [(1, -1), (1, 0), (1, 1), (0, 1)]:\n",
    "                    # loop on the 8 directions\n",
    "                    xx, yy = x, y\n",
    "                    for count in itertools.count():\n",
    "                        if not is_index(board, (xx, yy)) or \\\n",
    "                                board[xx, yy] != player:\n",
    "                            break\n",
    "                        xx, yy = xx + dx, yy + dy\n",
    "                    if count >= self.target_length:\n",
    "                        return player\n",
    "    for player in [BLACK, WHITE]:\n",
    "        if self.has_valid((board, player)):\n",
    "            return None\n",
    "    return 0\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9eb39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "def get_next_state(self, state, action):\n",
    "    \"\"\"\n",
    "    Get the next state.\n",
    "\n",
    "    Parameters\n",
    "    ----\n",
    "    state : (np.array, int)    board and current player\n",
    "    action : np.array    location and skip indicator\n",
    "\n",
    "    Returns\n",
    "    ----\n",
    "    next_state : (np.array, int)    next board and next player\n",
    "\n",
    "    Raise\n",
    "    ----\n",
    "    ValueError : location in action is not valid\n",
    "    \"\"\"\n",
    "    board, player = state\n",
    "    x, y = action\n",
    "    if self.is_valid(state, action):\n",
    "        board = copy.deepcopy(board)\n",
    "        board[x, y] = player\n",
    "    return board, -player\n",
    "\n",
    "def next_step(self, state, action):\n",
    "    \"\"\"\n",
    "    Get the next observation, reward, done, and info.\n",
    "\n",
    "    Parameters\n",
    "    ----\n",
    "    state : (np.array, int)    board and current player\n",
    "    action : np.array    location\n",
    "\n",
    "    Returns\n",
    "    ----\n",
    "    next_state : (np.array, int)    next board and next player\n",
    "    reward : float    the winner or zeros\n",
    "    done : bool    whether the game end or not\n",
    "    info : {'valid' : np.array} a dict shows the valid place for next player\n",
    "    \"\"\"\n",
    "    if not self.is_valid(state, action):\n",
    "        action = self.illegal_equivalent_action\n",
    "    if np.array_equal(action, self.RESIGN):\n",
    "        return state, -state[1], True, {}\n",
    "    while True:\n",
    "        state = self.get_next_state(state, action)\n",
    "        winner = self.get_winner(state)\n",
    "        if winner is not None:\n",
    "            return state, winner, True, {}\n",
    "        if self.has_valid(state):\n",
    "            break\n",
    "        action = self.PASS\n",
    "    return state, 0., False, {}\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "def reset(self, *, seed=None, return_info=False, options=None):\n",
    "    \"\"\"\n",
    "    Reset a new game episode. See gym.Env.reset()\n",
    "\n",
    "    Returns\n",
    "    ----\n",
    "    next_state : (np.array, int)    next board and next player\n",
    "    \"\"\"\n",
    "    super().reset(seed=seed)\n",
    "    self.board = np.zeros_like(self.board, dtype=np.int8)\n",
    "    self.player = BLACK\n",
    "    if not return_info:\n",
    "        return self.board, self.player\n",
    "    else:\n",
    "        return (self.board, self.player), {}\n",
    "\n",
    "def step(self, action):\n",
    "    \"\"\"\n",
    "    See gym.Env.step().\n",
    "\n",
    "    Parameters\n",
    "    ----\n",
    "    action : np.array    location\n",
    "\n",
    "    Returns\n",
    "    ----\n",
    "    next_state : (np.array, int)    next board and next player\n",
    "    reward : float                  the winner or zero\n",
    "    termination : bool              whether the game end or not\n",
    "    truncation : bool\n",
    "    info : {}\n",
    "    \"\"\"\n",
    "    state = (self.board, self.player)\n",
    "    next_state, reward, termination, info = self.next_step(state, action)\n",
    "    self.board, self.player = next_state\n",
    "    return next_state, reward, termination, truncation, info\n",
    "\n",
    "metadata = {\"render_modes\": [\"ansi\", \"human\"]}\n",
    "\n",
    "def render(self, mode='human'):\n",
    "    \"\"\"\n",
    "    See gym.Env.render().\n",
    "    \"\"\"\n",
    "    outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "    s = strfboard(self.board, self.render_characters)\n",
    "    outfile.write(s)\n",
    "    if mode != 'human':\n",
    "        return outfile\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c64d9",
   "metadata": {},
   "source": [
    "Chapter 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d25bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "import gym\n",
    "import pybullet_envs\n",
    "\n",
    "env = gym.make(\"HumanoidBulletEnv-v0\", new_step_api=True)\n",
    "\n",
    "import pybullet as p\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11125435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "01234567890123456789012345678901234567890123456789012345678901234567890123456789\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
