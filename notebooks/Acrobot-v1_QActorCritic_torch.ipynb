{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Action-Actor-Critic to Play Acrobot-v1\n",
    "\n",
    "PyTorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import pandas as pd\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as distributions\n",
    "torch.manual_seed(0)\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        stream=sys.stdout, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:55:30 [INFO] env: <AcrobotEnv<Acrobot-v1>>\n",
      "22:55:30 [INFO] action_space: Discrete(3)\n",
      "22:55:30 [INFO] observation_space: Box(-28.274333953857422, 28.274333953857422, (6,), float32)\n",
      "22:55:30 [INFO] reward_range: (-inf, inf)\n",
      "22:55:30 [INFO] metadata: {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 15}\n",
      "22:55:30 [INFO] _max_episode_steps: 500\n",
      "22:55:30 [INFO] _elapsed_steps: None\n",
      "22:55:30 [INFO] id: Acrobot-v1\n",
      "22:55:30 [INFO] entry_point: gym.envs.classic_control:AcrobotEnv\n",
      "22:55:30 [INFO] reward_threshold: -100.0\n",
      "22:55:30 [INFO] nondeterministic: False\n",
      "22:55:30 [INFO] max_episode_steps: 500\n",
      "22:55:30 [INFO] _kwargs: {}\n",
      "22:55:30 [INFO] _env_name: Acrobot\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "env.seed(0)\n",
    "for key in vars(env):\n",
    "    logging.info('%s: %s', key, vars(env)[key])\n",
    "for key in vars(env.spec):\n",
    "    logging.info('%s: %s', key, vars(env.spec)[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QActorCriticAgent:\n",
    "    def __init__(self, env):\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        self.actor_net = self.build_net(\n",
    "                input_size=env.observation_space.shape[0],\n",
    "                hidden_sizes=[100,],\n",
    "                output_size=env.action_space.n, output_activator=nn.Softmax(1))\n",
    "        self.actor_optimizer = optim.Adam(self.actor_net.parameters(), 0.001)\n",
    "        self.critic_net = self.build_net(\n",
    "                input_size=env.observation_space.shape[0],\n",
    "                hidden_sizes=[100,],\n",
    "                output_size=env.action_space.n)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_net.parameters(), 0.002)\n",
    "        self.critic_loss = nn.MSELoss()\n",
    "\n",
    "    def build_net(self, input_size, hidden_sizes, output_size=1,\n",
    "            output_activator=None):\n",
    "        layers = []\n",
    "        for input_size, output_size in zip(\n",
    "                [input_size,] + hidden_sizes, hidden_sizes + [output_size,]):\n",
    "            layers.append(nn.Linear(input_size, output_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers = layers[:-1]\n",
    "        if output_activator:\n",
    "            layers.append(output_activator)\n",
    "        net = nn.Sequential(*layers)\n",
    "        return net\n",
    "\n",
    "    def reset(self, mode=None):\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train':\n",
    "            self.trajectory = []\n",
    "            self.discount = 1.\n",
    "\n",
    "    def step(self, observation, reward, done):\n",
    "        state_tensor = torch.as_tensor(observation, dtype=torch.float).reshape(1, -1)\n",
    "        prob_tensor = self.actor_net(state_tensor)\n",
    "        action_tensor = distributions.Categorical(prob_tensor).sample()\n",
    "        action = action_tensor.numpy()[0]\n",
    "        if self.mode == 'train':\n",
    "            self.trajectory += [observation, reward, done, action]\n",
    "            if len(self.trajectory) >= 8:\n",
    "                self.learn()\n",
    "            self.discount *= self.gamma\n",
    "        return action\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        state, _, _, action, next_state, reward, done, next_action \\\n",
    "                = self.trajectory[-8:]\n",
    "        state_tensor = torch.as_tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "        next_state_tensor = torch.as_tensor(next_state,\n",
    "                dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "        # train actor\n",
    "        q_tensor = self.critic_net(state_tensor)[0, action]\n",
    "        pi_tensor = self.actor_net(state_tensor)[0, action]\n",
    "        logpi_tensor = torch.log(pi_tensor.clamp(1e-6, 1.))\n",
    "        actor_loss_tensor = -self.discount * q_tensor * logpi_tensor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss_tensor.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # train critic\n",
    "        next_q_tensor = self.critic_net(next_state_tensor)[:, next_action]\n",
    "        target_tensor = reward + (1. - done) * self.gamma * next_q_tensor\n",
    "        pred_tensor = self.critic_net(state_tensor)[:, action]\n",
    "        critic_loss_tensor = self.critic_loss(pred_tensor, target_tensor)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss_tensor.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "\n",
    "agent = QActorCriticAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:55:30 [INFO] ==== train ====\n",
      "22:55:34 [DEBUG] train episode 0: reward = -500.00, steps = 500\n",
      "22:55:39 [DEBUG] train episode 1: reward = -500.00, steps = 500\n",
      "22:55:41 [DEBUG] train episode 2: reward = -357.00, steps = 358\n",
      "22:55:45 [DEBUG] train episode 3: reward = -500.00, steps = 500\n",
      "22:55:50 [DEBUG] train episode 4: reward = -500.00, steps = 500\n",
      "22:55:54 [DEBUG] train episode 5: reward = -500.00, steps = 500\n",
      "22:55:58 [DEBUG] train episode 6: reward = -500.00, steps = 500\n",
      "22:56:02 [DEBUG] train episode 7: reward = -500.00, steps = 500\n",
      "22:56:06 [DEBUG] train episode 8: reward = -500.00, steps = 500\n",
      "22:56:11 [DEBUG] train episode 9: reward = -500.00, steps = 500\n",
      "22:56:15 [DEBUG] train episode 10: reward = -500.00, steps = 500\n",
      "22:56:20 [DEBUG] train episode 11: reward = -500.00, steps = 500\n",
      "22:56:24 [DEBUG] train episode 12: reward = -500.00, steps = 500\n",
      "22:56:28 [DEBUG] train episode 13: reward = -500.00, steps = 500\n",
      "22:56:32 [DEBUG] train episode 14: reward = -500.00, steps = 500\n",
      "22:56:36 [DEBUG] train episode 15: reward = -500.00, steps = 500\n",
      "22:56:41 [DEBUG] train episode 16: reward = -500.00, steps = 500\n",
      "22:56:46 [DEBUG] train episode 17: reward = -500.00, steps = 500\n",
      "22:56:51 [DEBUG] train episode 18: reward = -500.00, steps = 500\n",
      "22:56:56 [DEBUG] train episode 19: reward = -500.00, steps = 500\n",
      "22:57:01 [DEBUG] train episode 20: reward = -500.00, steps = 500\n",
      "22:57:06 [DEBUG] train episode 21: reward = -500.00, steps = 500\n",
      "22:57:10 [DEBUG] train episode 22: reward = -500.00, steps = 500\n",
      "22:57:15 [DEBUG] train episode 23: reward = -500.00, steps = 500\n",
      "22:57:20 [DEBUG] train episode 24: reward = -500.00, steps = 500\n",
      "22:57:25 [DEBUG] train episode 25: reward = -500.00, steps = 500\n",
      "22:57:29 [DEBUG] train episode 26: reward = -500.00, steps = 500\n",
      "22:57:34 [DEBUG] train episode 27: reward = -500.00, steps = 500\n",
      "22:57:38 [DEBUG] train episode 28: reward = -500.00, steps = 500\n",
      "22:57:42 [DEBUG] train episode 29: reward = -500.00, steps = 500\n",
      "22:57:46 [DEBUG] train episode 30: reward = -500.00, steps = 500\n",
      "22:57:50 [DEBUG] train episode 31: reward = -500.00, steps = 500\n",
      "22:57:54 [DEBUG] train episode 32: reward = -500.00, steps = 500\n",
      "22:57:58 [DEBUG] train episode 33: reward = -500.00, steps = 500\n",
      "22:58:02 [DEBUG] train episode 34: reward = -500.00, steps = 500\n",
      "22:58:06 [DEBUG] train episode 35: reward = -500.00, steps = 500\n",
      "22:58:10 [DEBUG] train episode 36: reward = -500.00, steps = 500\n",
      "22:58:14 [DEBUG] train episode 37: reward = -500.00, steps = 500\n",
      "22:58:17 [DEBUG] train episode 38: reward = -500.00, steps = 500\n",
      "22:58:20 [DEBUG] train episode 39: reward = -500.00, steps = 500\n",
      "22:58:23 [DEBUG] train episode 40: reward = -500.00, steps = 500\n",
      "22:58:26 [DEBUG] train episode 41: reward = -500.00, steps = 500\n",
      "22:58:27 [DEBUG] train episode 42: reward = -158.00, steps = 159\n",
      "22:58:27 [DEBUG] train episode 43: reward = -110.00, steps = 111\n",
      "22:58:28 [DEBUG] train episode 44: reward = -106.00, steps = 107\n",
      "22:58:29 [DEBUG] train episode 45: reward = -99.00, steps = 100\n",
      "22:58:29 [DEBUG] train episode 46: reward = -76.00, steps = 77\n",
      "22:58:30 [DEBUG] train episode 47: reward = -98.00, steps = 99\n",
      "22:58:30 [DEBUG] train episode 48: reward = -105.00, steps = 106\n",
      "22:58:31 [DEBUG] train episode 49: reward = -78.00, steps = 79\n",
      "22:58:31 [DEBUG] train episode 50: reward = -84.00, steps = 85\n",
      "22:58:32 [DEBUG] train episode 51: reward = -80.00, steps = 81\n",
      "22:58:32 [INFO] ==== test ====\n",
      "22:58:32 [DEBUG] test episode 0: reward = -89.00, steps = 90\n",
      "22:58:32 [DEBUG] test episode 1: reward = -103.00, steps = 104\n",
      "22:58:32 [DEBUG] test episode 2: reward = -109.00, steps = 110\n",
      "22:58:32 [DEBUG] test episode 3: reward = -83.00, steps = 84\n",
      "22:58:32 [DEBUG] test episode 4: reward = -103.00, steps = 104\n",
      "22:58:33 [DEBUG] test episode 5: reward = -84.00, steps = 85\n",
      "22:58:33 [DEBUG] test episode 6: reward = -85.00, steps = 86\n",
      "22:58:33 [DEBUG] test episode 7: reward = -102.00, steps = 103\n",
      "22:58:33 [DEBUG] test episode 8: reward = -74.00, steps = 75\n",
      "22:58:33 [DEBUG] test episode 9: reward = -92.00, steps = 93\n",
      "22:58:33 [DEBUG] test episode 10: reward = -101.00, steps = 102\n",
      "22:58:33 [DEBUG] test episode 11: reward = -107.00, steps = 108\n",
      "22:58:33 [DEBUG] test episode 12: reward = -110.00, steps = 111\n",
      "22:58:33 [DEBUG] test episode 13: reward = -99.00, steps = 100\n",
      "22:58:34 [DEBUG] test episode 14: reward = -98.00, steps = 99\n",
      "22:58:34 [DEBUG] test episode 15: reward = -123.00, steps = 124\n",
      "22:58:34 [DEBUG] test episode 16: reward = -103.00, steps = 104\n",
      "22:58:34 [DEBUG] test episode 17: reward = -93.00, steps = 94\n",
      "22:58:34 [DEBUG] test episode 18: reward = -82.00, steps = 83\n",
      "22:58:34 [DEBUG] test episode 19: reward = -85.00, steps = 86\n",
      "22:58:34 [DEBUG] test episode 20: reward = -87.00, steps = 88\n",
      "22:58:34 [DEBUG] test episode 21: reward = -123.00, steps = 124\n",
      "22:58:34 [DEBUG] test episode 22: reward = -93.00, steps = 94\n",
      "22:58:35 [DEBUG] test episode 23: reward = -102.00, steps = 103\n",
      "22:58:35 [DEBUG] test episode 24: reward = -251.00, steps = 252\n",
      "22:58:35 [DEBUG] test episode 25: reward = -136.00, steps = 137\n",
      "22:58:35 [DEBUG] test episode 26: reward = -109.00, steps = 110\n",
      "22:58:35 [DEBUG] test episode 27: reward = -76.00, steps = 77\n",
      "22:58:35 [DEBUG] test episode 28: reward = -95.00, steps = 96\n",
      "22:58:35 [DEBUG] test episode 29: reward = -70.00, steps = 71\n",
      "22:58:35 [DEBUG] test episode 30: reward = -88.00, steps = 89\n",
      "22:58:36 [DEBUG] test episode 31: reward = -106.00, steps = 107\n",
      "22:58:36 [DEBUG] test episode 32: reward = -86.00, steps = 87\n",
      "22:58:36 [DEBUG] test episode 33: reward = -72.00, steps = 73\n",
      "22:58:36 [DEBUG] test episode 34: reward = -97.00, steps = 98\n",
      "22:58:36 [DEBUG] test episode 35: reward = -136.00, steps = 137\n",
      "22:58:36 [DEBUG] test episode 36: reward = -107.00, steps = 108\n",
      "22:58:36 [DEBUG] test episode 37: reward = -109.00, steps = 110\n",
      "22:58:36 [DEBUG] test episode 38: reward = -109.00, steps = 110\n",
      "22:58:36 [DEBUG] test episode 39: reward = -99.00, steps = 100\n",
      "22:58:37 [DEBUG] test episode 40: reward = -86.00, steps = 87\n",
      "22:58:37 [DEBUG] test episode 41: reward = -139.00, steps = 140\n",
      "22:58:37 [DEBUG] test episode 42: reward = -113.00, steps = 114\n",
      "22:58:37 [DEBUG] test episode 43: reward = -500.00, steps = 500\n",
      "22:58:37 [DEBUG] test episode 44: reward = -94.00, steps = 95\n",
      "22:58:37 [DEBUG] test episode 45: reward = -95.00, steps = 96\n",
      "22:58:38 [DEBUG] test episode 46: reward = -82.00, steps = 83\n",
      "22:58:38 [DEBUG] test episode 47: reward = -84.00, steps = 85\n",
      "22:58:38 [DEBUG] test episode 48: reward = -92.00, steps = 93\n",
      "22:58:38 [DEBUG] test episode 49: reward = -500.00, steps = 500\n",
      "22:58:39 [DEBUG] test episode 50: reward = -111.00, steps = 112\n",
      "22:58:39 [DEBUG] test episode 51: reward = -83.00, steps = 84\n",
      "22:58:39 [DEBUG] test episode 52: reward = -97.00, steps = 98\n",
      "22:58:39 [DEBUG] test episode 53: reward = -116.00, steps = 117\n",
      "22:58:39 [DEBUG] test episode 54: reward = -91.00, steps = 92\n",
      "22:58:39 [DEBUG] test episode 55: reward = -90.00, steps = 91\n",
      "22:58:39 [DEBUG] test episode 56: reward = -86.00, steps = 87\n",
      "22:58:39 [DEBUG] test episode 57: reward = -95.00, steps = 96\n",
      "22:58:39 [DEBUG] test episode 58: reward = -111.00, steps = 112\n",
      "22:58:40 [DEBUG] test episode 59: reward = -67.00, steps = 68\n",
      "22:58:40 [DEBUG] test episode 60: reward = -82.00, steps = 83\n",
      "22:58:40 [DEBUG] test episode 61: reward = -80.00, steps = 81\n",
      "22:58:40 [DEBUG] test episode 62: reward = -81.00, steps = 82\n",
      "22:58:40 [DEBUG] test episode 63: reward = -64.00, steps = 65\n",
      "22:58:40 [DEBUG] test episode 64: reward = -109.00, steps = 110\n",
      "22:58:40 [DEBUG] test episode 65: reward = -106.00, steps = 107\n",
      "22:58:40 [DEBUG] test episode 66: reward = -96.00, steps = 97\n",
      "22:58:40 [DEBUG] test episode 67: reward = -118.00, steps = 119\n",
      "22:58:41 [DEBUG] test episode 68: reward = -156.00, steps = 157\n",
      "22:58:41 [DEBUG] test episode 69: reward = -91.00, steps = 92\n",
      "22:58:41 [DEBUG] test episode 70: reward = -107.00, steps = 108\n",
      "22:58:41 [DEBUG] test episode 71: reward = -74.00, steps = 75\n",
      "22:58:41 [DEBUG] test episode 72: reward = -93.00, steps = 94\n",
      "22:58:41 [DEBUG] test episode 73: reward = -81.00, steps = 82\n",
      "22:58:42 [DEBUG] test episode 74: reward = -500.00, steps = 500\n",
      "22:58:42 [DEBUG] test episode 75: reward = -98.00, steps = 99\n",
      "22:58:42 [DEBUG] test episode 76: reward = -200.00, steps = 201\n",
      "22:58:42 [DEBUG] test episode 77: reward = -106.00, steps = 107\n",
      "22:58:42 [DEBUG] test episode 78: reward = -102.00, steps = 103\n",
      "22:58:42 [DEBUG] test episode 79: reward = -183.00, steps = 184\n",
      "22:58:42 [DEBUG] test episode 80: reward = -102.00, steps = 103\n",
      "22:58:43 [DEBUG] test episode 81: reward = -75.00, steps = 76\n",
      "22:58:43 [DEBUG] test episode 82: reward = -100.00, steps = 101\n",
      "22:58:43 [DEBUG] test episode 83: reward = -95.00, steps = 96\n",
      "22:58:43 [DEBUG] test episode 84: reward = -76.00, steps = 77\n",
      "22:58:43 [DEBUG] test episode 85: reward = -131.00, steps = 132\n",
      "22:58:43 [DEBUG] test episode 86: reward = -116.00, steps = 117\n",
      "22:58:43 [DEBUG] test episode 87: reward = -91.00, steps = 92\n",
      "22:58:43 [DEBUG] test episode 88: reward = -99.00, steps = 100\n",
      "22:58:44 [DEBUG] test episode 89: reward = -85.00, steps = 86\n",
      "22:58:44 [DEBUG] test episode 90: reward = -90.00, steps = 91\n",
      "22:58:44 [DEBUG] test episode 91: reward = -115.00, steps = 116\n",
      "22:58:44 [DEBUG] test episode 92: reward = -90.00, steps = 91\n",
      "22:58:44 [DEBUG] test episode 93: reward = -77.00, steps = 78\n",
      "22:58:44 [DEBUG] test episode 94: reward = -99.00, steps = 100\n",
      "22:58:44 [DEBUG] test episode 95: reward = -121.00, steps = 122\n",
      "22:58:44 [DEBUG] test episode 96: reward = -79.00, steps = 80\n",
      "22:58:44 [DEBUG] test episode 97: reward = -98.00, steps = 99\n",
      "22:58:45 [DEBUG] test episode 98: reward = -89.00, steps = 90\n",
      "22:58:45 [DEBUG] test episode 99: reward = -107.00, steps = 108\n",
      "22:58:45 [INFO] average episode reward = -112.70 ± 72.79\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def play_episode(env, agent, max_episode_steps=None, mode=None, render=False):\n",
    "    observation, reward, done = env.reset(), 0., False\n",
    "    agent.reset(mode=mode)\n",
    "    episode_reward, elapsed_steps = 0., 0\n",
    "    while True:\n",
    "        action = agent.step(observation, reward, done)\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        elapsed_steps += 1\n",
    "        if max_episode_steps and elapsed_steps >= max_episode_steps:\n",
    "            break\n",
    "    agent.close()\n",
    "    return episode_reward, elapsed_steps\n",
    "\n",
    "\n",
    "logging.info('==== train ====')\n",
    "episode_rewards = []\n",
    "for episode in itertools.count():\n",
    "    episode_reward, elapsed_steps = play_episode(env.unwrapped, agent,\n",
    "            max_episode_steps=env._max_episode_steps, mode='train')\n",
    "    episode_rewards.append(episode_reward)\n",
    "    logging.debug('train episode %d: reward = %.2f, steps = %d',\n",
    "            episode, episode_reward, elapsed_steps)\n",
    "    if np.mean(episode_rewards[-10:]) > -120:\n",
    "        break\n",
    "plt.plot(episode_rewards)\n",
    "\n",
    "\n",
    "logging.info('==== test ====')\n",
    "episode_rewards = []\n",
    "for episode in range(100):\n",
    "    episode_reward, elapsed_steps = play_episode(env, agent)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    logging.debug('test episode %d: reward = %.2f, steps = %d',\n",
    "            episode, episode_reward, elapsed_steps)\n",
    "logging.info('average episode reward = %.2f ± %.2f',\n",
    "        np.mean(episode_rewards), np.std(episode_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
