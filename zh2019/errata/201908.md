# 《强化学习：原理与Python实现》更新与勘误

（2019年8月第1版第1次印刷）

Gym 0.25版本更新了API，GitHub代码已相应更新。代码以GitHub上为准。

### 行数计算方法

本勘误文档中，行数计算“第$i$行”（$i=0,1,2,\ldots$）是从0开始计数的。小节标题、公式、内联代码、注意、本章要点记入行数，章标题、图、表、代码清单及它们的题注不计入行数。空行不计入行数。

$
\newcommand{\sfA}{\unicode{x1d608}}
\newcommand{\sfR}{\unicode{x1d619}}
\newcommand{\sfS}{\unicode{x1d61a}}
\newcommand{\sfa}{\unicode{x1d622}}
\newcommand{\sfs}{\unicode{x1d634}}
\newcommand{\sfx}{\unicode{x1d639}}
\newcommand{\bftheta}{\pmb{\unicode{x3B8}}}
\newcommand{\E}{\textrm{E}}
$

## 第5页第19行

记为$\sfR $

### 改为

记为$R$

## 第11页第17行

动作空间是Dicrete(3)

### 改为

动作空间是Discrete(3)


## 第17页第6行

作者注：
“轨道”又称为“轨迹”。本书中这两个词混用。


## 第20页第2行

作者注：
这种带折扣的回报定义既可以用于回合制任务，也可以用于连续性任务，是一种统一的表示。


## 第20页第9行

为$\bar{R}=\lim\limits_{t\to+\infty}\E\left[\frac{1}{t}\sum\limits_{\tau=0}^{t}R_\tau\right]$

### 改为

为$\bar{R}=\lim\limits_{t\to+\infty}\E\left[\frac{1}{t}\sum\limits_{\tau=1}^{t}R_\tau\right]$


## 第20页第24-27行

$q_\pi\left(吃\mid饿\right)=\E_\pi\left[G_t\mid\sfS_t=饿,\sfA_t=吃\right]$

$q_\pi\left(不吃\mid饿\right)=\E_\pi\left[G_t\mid\sfS_t=饿,\sfA_t=不吃\right]$

$q_\pi\left(吃\mid饱\right)=\E_\pi\left[G_t\mid\sfS_t=饱,\sfA_t=吃\right]$

$q_\pi\left(不吃\mid饱\right)=\E_\pi\left[G_t\mid\sfS_t=饱,\sfA_t=不吃\right]$

### 改为

$q_\pi\left(饿,吃\right)=\E_\pi\left[G_t\mid\sfS_t=饿,\sfA_t=吃\right]$

$q_\pi\left(饿,不吃\right)=\E_\pi\left[G_t\mid\sfS_t=饿,\sfA_t=不吃\right]$

$q_\pi\left(饱,吃\right)=\E_\pi\left[G_t\mid\sfS_t=饱,\sfA_t=吃\right]$

$q_\pi\left(饱,不吃\right)=\E_\pi\left[G_t\mid\sfS_t=饱,\sfA_t=不吃\right]$

## 第22页图2-2b)中图注

$v_\pi(\sfs)$

#### 改为

$v_\pi(\sfs')$

## 第2.2节和第2.3节

有观点认为：用动作价值表示状态价值的表达式，或是用状态价值表示动作价值的表达式，不能称为Bellman方程；而用动作价值表示动作价值的表达式，或是用状态价值表示状态价值的表达式，才可以称为Bellman方程。

## 第22页第18行

${q_\pi}\left(\sfs,\sfa\right)=\gamma \sum\limits_{\sfs',r}{p\left(\sfs',r\mid\sfs,\sfa\right)\left[r+\sum\limits_{\sfa'}{\pi\left(\sfa'|\sfs'\right){q_\pi}\left(\sfs',\sfa'\right)}\right]}$

### 改为

${q_\pi}\left(\sfs,\sfa\right)=\sum\limits_{\sfs',r}{p\left(\sfs',r\mid\sfs,\sfa\right)\left[r+\gamma \sum\limits_{\sfa'}{\pi\left(\sfa'|\sfs'\right){q_\pi}\left(\sfs',\sfa'\right)}\right]}$

## 第23页第19行

$\begin{bmatrix}1&0&x-1&-x&0&0\\\\0&1&0&0&-y&y-1\\\\-\gamma&0&1&0&0&0\\\\\left(\alpha-1\right)\gamma&-\alpha\gamma&0&1&0&0\\\\-\beta\gamma&\left(\beta-1\right)\gamma&0&0&1&0\\\\0&-\gamma&0&0&0&1\end{bmatrix}\begin{bmatrix}v_\pi\left(饿\right)\\\\v_\pi\left(饱\right)\\\\q_\pi\left(饿,不吃\right)\\\\q_\pi\left(饿,吃\right)\\\\q_\pi\left(饱,不吃\right)\\\\q_\pi\left(饱,吃\right)\end{bmatrix}=\begin{bmatrix}0\\\\0\\\\2\\\\-4\alpha+3\\\\4\beta-2\\\\-1\end{bmatrix}$

### 改为

$\begin{bmatrix}1&0&x-1&-x&0&0\\\\0&1&0&0&-y&y-1\\\\-\gamma&0&1&0&0&0\\\\\left(\alpha-1\right)\gamma&-\alpha\gamma&0&1&0&0\\\\-\beta\gamma&\left(\beta-1\right)\gamma&0&0&1&0\\\\0&-\gamma&0&0&0&1\end{bmatrix}\begin{bmatrix}v_\pi\left(饿\right)\\\\v_\pi\left(饱\right)\\\\q_\pi\left(饿,不吃\right)\\\\q_\pi\left(饿,吃\right)\\\\q_\pi\left(饱,不吃\right)\\\\q_\pi\left(饱,吃\right)\end{bmatrix}=\begin{bmatrix}0\\\\0\\\\-2\\\\4\alpha-3\\\\-4\beta+2\\\\1\end{bmatrix}$

## 第24页代码清单2-1

### 代码改为

```
import sympy
from sympy import symbols
sympy.init_printing()
v_hungry, v_full = symbols('v_hungry v_full')
q_hungry_eat, q_hungry_none, q_full_eat, q_full_none = \
        symbols('q_hungry_eat q_hungry_none q_full_eat q_full_none')
alpha, beta, x, y, gamma = symbols('alpha beta x y gamma')
system = sympy.Matrix((
        (1, 0, x-1, -x, 0, 0, 0),
        (0, 1, 0, 0, -y, y-1, 0),
        (-gamma, 0, 1, 0, 0, 0, -2),
        ((alpha-1)*gamma, -alpha*gamma, 0, 1, 0, 0, 4*alpha-3),
        (-beta*gamma, (beta-1)*gamma, 0, 0, 1, 0, -4*beta+2),
        (0, -gamma, 0, 0, 0, 1, 1) )) # 标准形式的系数矩阵
sympy.solve_linear_system(system,
        v_hungry, v_full,
        q_hungry_none, q_hungry_eat, q_full_none, q_full_eat)
```

## 第24页第2行~第11行

### 求解有误，更改如下

$v_\pi\left(饿\right)=\frac{1}{\Delta}\left(\alpha\gamma xy-3\alpha\gamma x+4\alpha x-\beta\gamma xy-2\beta\gamma y+\gamma x+2\gamma-x-2\right)$

$v_\pi\left(饱\right)=\frac{1}{\Delta}\left(\alpha\gamma xy+\alpha\gamma x-\beta\gamma xy+2\beta\gamma y-4\beta y-\gamma y-\gamma+y+1\right)$

$q_\pi\left(饿,不吃\right)=\frac{1}{\Delta}\left(\alpha\gamma^2xy-\alpha\gamma^2x+2\alpha\gamma x-\beta\gamma^2xy-2\beta\gamma y+\gamma^2x-\gamma x+2\gamma-2\right)$

$q_\pi\left(饿,吃\right)=\frac{1}{\Delta}\left(\alpha\gamma^2xy-\alpha\gamma^2x-\alpha\gamma^2y+\alpha\gamma^2+2\alpha\gamma x+\alpha\gamma y-5\alpha\gamma+4\alpha-\beta\gamma^2xy+\beta\gamma^2y-\right.$

$\left.3\beta\gamma y+\gamma^2x-\gamma^2-\gamma x+4\gamma-3\right)$

$q_\pi\left(饱,不吃\right)=\frac{1}{\Delta}\left(\alpha\gamma^2xy-\alpha\gamma^2x+2\alpha\gamma x-\beta\gamma^2xy+\beta\gamma^2x+\beta\gamma^2y-\beta\gamma^2-\beta\gamma x-3\beta\gamma y+\right.$

$\left.5\beta\gamma-4\beta-\gamma^2{y}+\gamma^2+\gamma{y}-3\gamma+2\right)$

$q_\pi\left(饱,吃\right)=\frac{1}{\Delta}\left(\alpha\gamma^2xy+\alpha\gamma x-\beta\gamma^2xy+\beta\gamma^2y-3\beta\gamma y-\gamma^2y+\gamma y-\gamma+1\right)$

其中

$\Delta=\left(1-\gamma\right)\left(1-\left(1-\alpha{x}-\beta{y}\right)\gamma\right)$

## 第25页第2行

如果对于任意$\sfs\in\mathcal{S}$，都${v_{\pi}}\left(\sfs\right)\le{v_*}\left(\sfs\right)$，则称

### 改为

如果对于任意$\sfs\in\mathcal{S}$都满足${v_{\pi}}\left(\sfs\right)\le{v_*}\left(\sfs\right)$，则称

## 第26页第2行和第7行

$\gamma\sum_{\sfs'}{p\left(\sfs',r\mid\sfs,\sfa\right)}v_*\left(\sfs'\right)$

（共2处）

### 改为

$\gamma\sum_{\sfs'}{p\left(\sfs'\mid\sfs,\sfa\right)}v_*\left(\sfs'\right)$

## 第27页第0~5行

所有的下标$\pi$改为下标$\ast$（共16处）。


## 第27页代码清单2-2

### 代码改为

```
import sympy
from sympy import symbols
sympy.init_printing()
alpha, beta, gamma = symbols('alpha beta gamma')
v_hungry, v_full = symbols('v_hungry v_full')
q_hungry_eat, q_hungry_none, q_full_eat, q_full_none = \
        symbols('q_hungry_eat q_hungry_none q_full_eat q_full_none')
xy_tuples = ((0, 0), (1, 0), (0, 1), (1, 1))
for x, y in xy_tuples: # 分类讨论
    system = sympy.Matrix((
            (1, 0, x-1, -x, 0, 0, 0),
            (0, 1, 0, 0, -y, y-1, 0),
            (-gamma, 0, 1, 0, 0, 0, -2),
            ((alpha-1)*gamma, -alpha*gamma, 0, 1, 0, 0, 4*alpha-3),
            (-beta*gamma, (beta-1)*gamma, 0, 0, 1, 0, -4*beta+2),
            (0, -gamma, 0, 0, 0, 1, 1) ))
    result = sympy.solve_linear_system(system,
            v_hungry, v_full,
            q_hungry_none, q_hungry_eat, q_full_none, q_full_eat, simplification=True)
    msgx = 'v(饿) = q(饿,{}吃)'.format('' if x else '不')
    msgy = 'v(饱) = q(饱,{}吃)'.format('不' if y else '')
    print('==== {}, {} ==== x = {}, y = {} ===='.format(msgx, msgy, x, y))
    display(result)
```

## 第27页第10行~第29页第8行

### 求解有误，更改如下

接下来进一步分析这个方程组。比较最优价值的方程组和一般策略的价值方程组可以发现，最优策略价值方程组的解正是在一般策略方程组的解中$\left(x,y\right)$分别取(0,0)、(0,1)、(1,0)、(1,1)得到。所以，我们用一般策略价值方程组的解的形式来表示最优价值方程组的解，并比较其中$q_\ast\left(\right.饱$,$不吃\left.\right)$和$q_\ast\left(饱,吃\right)$的大小，以及$q_\ast\left(饿,不吃\right)$和$q_\ast\left(饿,吃\right)$的大小。在比较时，注意到$x,y\in\left[0,1\right]$并且已经假设$\alpha,\beta,\gamma\in\left(0,1\right)$。所以可以验证

$\Delta=\left(1-\gamma\right)\left(1-\left(1-\alpha{x}-\beta{y}\right)\gamma\right)>0$

进而比较大小可以只比较分子部分。

首先来比较$q_\ast\left(饿,不吃\right)$和$q_\ast\left(饿,吃\right)$的大小。注意到$q_\ast\left(饿,不吃\right)\le{q_\ast}\left(饿,不吃\right)$等价于

$\alpha\gamma^2xy-\alpha\gamma^2x+2\alpha\gamma x-\beta\gamma^2xy-2\beta\gamma y+\gamma^2x-\gamma x+2\gamma-2\le$

$\alpha\gamma^2xy-\alpha\gamma^2x-\alpha\gamma^2y+\alpha\gamma^2+2\alpha\gamma x+\alpha\gamma y-5\alpha\gamma+4\alpha-\beta\gamma^2xy+\beta\gamma^2y-3\beta\gamma y+\gamma^2x-\gamma^2-\gamma x+4\gamma-3$

即

$-\alpha\gamma^2y+\alpha\gamma^2+\alpha\gamma y-5\alpha\gamma+4\alpha+\beta\gamma^2y-\beta\gamma y-\gamma^2+2\gamma-1\ge0$

两边除以$\left(1-\gamma\right)$可得

$\alpha\gamma{y}-\alpha\gamma+4\alpha-\beta\gamma y+\gamma-1\ge0$

注意到$1-\alpha+\left(\alpha-\beta\right)y>0$，上述不等式等价于

$\gamma\ge\frac{1-4\alpha}{1-\alpha+\left(\alpha-\beta\right)y}$.

接下来比较$q_\ast\left(饱,不吃\right)$和$q_\ast\left(饱,吃\right)$的大小。注意到$q_\ast\left(饱,不吃\right)\le{q_\ast}\left(饱,吃\right)$等价于
$\alpha{\gamma^2}xy-\alpha{\gamma^2}x+2\alpha\gamma{x}-\beta{\gamma^2}xy+\beta{\gamma^2}x+\beta{\gamma^2}y-\beta{\gamma^2}-\beta\gamma{x}-3\beta\gamma{y}+5\beta\gamma-4\beta-{\gamma^2}y+\gamma^2+\gamma{y}-3\gamma+2\le$

$\alpha{\gamma^2}xy+\alpha\gamma{x}-\beta{\gamma^2}xy+\beta{\gamma^2}y-3\beta\gamma{y}-{\gamma^2}y+\gamma{y}-\gamma+1$

即

$\alpha{\gamma^2}x-\alpha\gamma{x}-\beta{\gamma^2}x+\beta\gamma{x}+\beta\gamma^2-5\beta\gamma+4\beta-\gamma^2+2\gamma-1\ge{0}$

两边除以$\left(1-\gamma\right)$可得

$-\alpha\gamma{x}+\beta\gamma{x}-\beta\gamma+4\beta+\gamma-1\ge{0}$

注意到$1-\beta+\left(\beta-\alpha\right)x>0$，上述不等式等价于

$\gamma\ge\frac{1-4\beta}{1-\beta+\left(\beta-\alpha\right)x}$.

综合上述分析，有以下四种情况。

**情况I**：$q_\ast\left(饿,不吃\right)>{q_\ast}\left(饿,吃\right)$且$q_\ast\left(饱,不吃\right)\le{q_\ast}\left(饱,吃\right)$。这时有$v_\ast\left(饿\right)=q_\ast\left(饿,不吃\right)$且$v_\ast\left(饱\right)=q_\ast\left(饱,吃\right)$，以及$x=0$且$y=0$。相应条件简化为$\gamma<\frac{1-4\alpha}{1-\alpha}$且$\gamma\ge\frac{1-4\beta}{1-\beta}$，最优价值简化为

$q_\ast\left(饿,吃\right)=\frac{1}{1-\gamma}\left(-\alpha\gamma+4\alpha+\gamma-3\right)$

$v_\ast\left(饿\right)=q_\ast\left(饿,不吃\right)=\frac{-2}{1-\gamma}$

$v_\ast\left(饱\right)=q_\ast\left(饱,吃\right)=\frac{1}{1-\gamma}$

$q_\ast\left(饱,不吃\right)=\frac{1}{1-\gamma}\left(\beta\gamma+4\beta-\gamma+2\right)$

**情况II**：$q_\ast\left(饿,不吃\right)\le{q_\ast}\left(饿,吃\right)$且$q_\ast\left(饱,不吃\right)\le{q_\ast}\left(饱,吃\right)$。这时有$v_\ast\left(饿\right)=q_\ast\left(饿,吃\right)$且$v_\ast\left(饱\right)=q_\ast\left(饱,吃\right)$，以及$x=1$且$y=0$。相应条件简化为$\gamma\ge\frac{1-4\alpha}{1-\alpha}$且$\gamma\ge\frac{1-4\beta}{1-\alpha}$，最优价值简化为

$q_\ast\left(饿,不吃\right)=\frac{1}{\Delta_\mathrm{II}}\left(-\alpha\gamma^2+2\alpha\gamma+\gamma^2+\gamma-2\right)$

$v_\ast\left(饿\right)=q_\ast\left(饿,吃\right)=\frac{1}{\Delta_\mathrm{II}}\left(-3\alpha\gamma+4\alpha+3\gamma-3\right)$

$q_\ast\left(饱,不吃\right)=\frac{1}{\Delta_\mathrm{II}}\left(-\alpha\gamma^2+2\alpha\gamma+4\beta\gamma-4\beta+\gamma^2-3\gamma+2\right)$

$v_\ast\left(饱\right)=q_\ast\left(饱,吃\right)=\frac{1}{1-\gamma}$

其中

$\Delta_\mathrm{II}=\left(1-\gamma\right)\left(1-\left(1-\alpha\right)\gamma\right)$.

**情况III**：$q_\ast\left(饿,不吃\right)>{q_\ast}\left(饿,吃\right)$且$q_\ast\left(饱,不吃\right)>{q_\ast}\left(饱,吃\right)$。这时有$v_\ast\left(饿\right)=q_\ast\left(饿,不吃\right)$且$v_\ast\left(饱\right)=q_\ast\left(饱,不吃\right)$，以及$x=0$且$y=1$。相应条件简化为$\gamma<\frac{1-4\alpha}{1-\beta}$且$\gamma<\frac{1-4\beta}{1-\beta}$，最优价值简化为

$v_\ast\left(饿\right)=q_\ast\left(饿,不吃\right)=\frac{-2}{1-\gamma}$

$q_\ast\left(饿,吃\right)=\frac{1}{\Delta_\mathrm{III}}\left(-4\alpha\gamma+4\alpha+\beta\gamma^2-3\beta\gamma-\gamma^2+4\gamma-3\right)$

$v_\ast\left(饱\right)=q_\ast\left(饱,不吃\right)=\frac{1}{\Delta_\mathrm{III}}\left(2\beta\gamma-4\beta-2\gamma+2\right)$

$q_\ast\left(饱,吃\right)=\frac{1}{\Delta_\mathrm{III}}\left(\beta\gamma^2-3\beta\gamma-\gamma^2+1\right)$

其中

$\Delta_\mathrm{III}=\left(1-\gamma\right)\left(1-\left(1-\beta\right)\gamma\right)$.

**情况IV**：$q_\ast\left(饿,不吃\right)\le{q_\ast}\left(饿,吃\right)$且$q_\ast\left(饱,不吃\right)>{q_\ast}\left(饱,吃\right)$。这时有$v_\ast\left(饿\right)=q_\ast\left(饿,吃\right)$且$v_\ast\left(饱\right)=q_\ast\left(饱,不吃\right)$，以及$x=1$且$y=1$。相应条件简化为$\gamma\ge\frac{1-4\alpha}{1-\beta}$且$\gamma<\frac{1-4\beta}{1-\alpha}$，最优价值简化为

$q_\ast\left(饿,不吃\right)=\frac{1}{\Delta_\mathrm{IV}}{\left(2\alpha\gamma-\beta\gamma^2-2\beta\gamma+\gamma^2+\gamma-2\right)}$

$v_\ast\left(饿\right)=q_\ast\left(饿,吃\right)=\frac{1}{\Delta_\mathrm{IV}}{\left(-2\alpha\gamma+4\alpha-3\beta\gamma+3\gamma-3\right)}$

$v_\ast\left(饱\right)=q_\ast\left(饱,不吃\right)=\frac{1}{\Delta_\mathrm{IV}}{\left(2\alpha\gamma+\beta\gamma-4\beta-2\gamma+2\right)}$。

$q_\ast\left(饱,吃\right)=\frac{1}{\Delta_\mathrm{IV}}{\left(\alpha\gamma^2+\alpha\gamma-3\beta\gamma-\gamma^2+1\right)}$

其中

$\Delta_\mathrm{IV}=\left(1-\gamma\right)\left(1-\left(1-\alpha-\beta\right)\gamma\right)$.


## 第29页第13行

$v\left(\sfs\right)\ge r\left(\sfs,\sfa\right)+\gamma\sum\limits_{\sfs'} {p\left(\sfs'\mid\sfs,\sfa\right){v_*}\left(\sfs'\right)},\quad\sfs\in\mathcal{S}$

### 改为

$v\left(\sfs\right)\ge r\left(\sfs,\sfa\right)+\gamma\sum\limits_{\sfs'}{p\left(\sfs'\mid\sfs,\sfa \right){v}\left(\sfs'\right)},\quad\sfs\in\mathcal{S},\sfa\in\mathcal{A}$

作者注：用线性规划求解最优状态价值的详细证明可见《Markov Decision Processes: Discrete Stochastic Dynamic Programming》（Martin Puterman）的第6.9节。证明大致如下：当$c\left(\sfs\right)$为Markov决策过程的初始状态分布时，原问题和对偶问题的目标都是Markov决策过程的平均回合奖励，原问题的最优解是最优状态价值。对$c\left(\sfs\right)$做灵敏度分析可知，无论$c\left(\sfs\right)$取什么分布，对偶问题均有解（这利用到对偶问题的可行域就是带折扣的状态动作对组成的分布所在的空间），所以原问题的最优解都是不变的。也就是说，Markov决策过程的最优状态价值和初始状态分布无关。进一步，可以在原问题中放宽$\sum\nolimits_{\sfs}c\left(\sfs\right)=1$这个限制，原问题的解依然不变，只是优化目标进行了放缩。


## 第29页第21行

$v_\pi\left(饿\right)=10$，$v_\pi\left(饱\right)=8\frac{3}{4}$

### 改为

$v_\ast\left(饿\right)=\frac{35}{11}$，$v_\ast\left(饱\right)=5$

## 第29页第23行

$q_\pi\left(饿,不吃\right)=7\frac{2}{3}$，$q_\pi\left(饿,吃\right)=10$，$q_\pi\left(饱,不吃\right)=6$，$q_\pi\left(饱,吃\right)=8\frac{3}{4}$

### 改为

$q_\ast\left(饿,不吃\right)=\frac{6}{11}$，$q_\ast\left(饿,吃\right)=\frac{35}{11}$，$q_\ast\left(饱,不吃\right)=\frac{21}{11}$，$q_\ast\left(饱,吃\right)=5$

## 第30页第4行~第25行

### 求解有误，更改如下

**情况I**：$q_\ast\left(饿,不吃\right)>{q_\ast}\left(饿,吃\right)$且$q_\ast\left(饱,不吃\right)\le{q_\ast}\left(饱,吃\right)$，即$\gamma<\frac{1-4\alpha}{1-\alpha}$且$\gamma\ge\frac{1-4\beta}{1-\beta}$。这种情况的最优策略是

$\pi_\ast\left(饿\right)=不吃$，$\pi_\ast\left(饱\right)=吃$.

即饿时不吃，饱时吃。

**情况II**：$q_\ast\left(饿,不吃\right)\le{q_\ast}\left(饿,吃\right)$且$q_\ast\left(饱,不吃\right)\le{q_\ast}\left(饱,吃\right)$，即$\gamma\ge\frac{1-4\alpha}{1-\alpha}$且$\gamma\ge\frac{1-4\beta}{1-\alpha}$。这种情况的最优策略是

$\pi_\ast\left(饿\right)=吃$，$\pi_\ast\left(饱\right)=吃$.

即一直吃。

**情况III**：$q_\ast\left(饿,不吃\right)>{q_\ast}\left(饿,吃\right)$且$q_\ast\left(饱,不吃\right)>{q_\ast}\left(饱,吃\right)$，即$\gamma<\frac{1-4\alpha}{1-\beta}$且$\gamma<\frac{1-4\beta}{1-\beta}$。这种情况的最优策略是

$\pi_\ast\left(饿\right)=不吃$，$\pi_\ast\left(饱\right)=不吃$.

即一直$不吃$。

**情况IV**：$q_\ast\left(饿,不吃\right)\le{q_\ast}\left(饿,吃\right)$且$q_\ast\left(饱,不吃\right)>{q_\ast}\left(饱,吃\right)$，即$\gamma\ge\frac{1-4\alpha}{1-\beta}$且$\gamma<\frac{1-4\beta}{1-\alpha}$。这种情况的最优策略是

$\pi_\ast\left(饿\right)=吃$，$\pi_\ast\left(饱\right)=不吃$.

即$饿$时$吃$，$饱$时$不吃$。

对于一组特定的数值，求解则更加直接。例如，当$\alpha=\frac{2}{3}$，$\beta=\frac{3}{4}$，$\gamma=\frac{4}{5}$时，2.3.2节已经求得了最优动作价值，此时动作价值满足$q_\ast\left(饿,不吃\right)<q_\ast\left(饿,吃\right)$且$q_\ast\left(饱,不吃\right)<q_\ast\left(饱,吃\right)$。所以，它对应的最优策略为$\pi_\ast\left(饿\right)=\pi_\ast\left(饱\right)=吃$.

## 第31页第12行

回合奖励为各步奖励之。

### 改为

回合奖励为各步奖励之和。

## 第32页代码清单2-4

### 代码改为

```
def play_once(env, policy):
    total_reward = 0
    state = env.reset()
    while True:
        loc = np.unravel_index(state, env.shape)
        print('状态 = {}, 位置 = {}'.format(state, loc), end=' ')
        action = np.random.choice(env.nA, p=policy[state])
        state, reward, done, _ = env.step(action)
        print('动作 = {}, 奖励 = {}'.format(action, reward))
        total_reward += reward
        if done:
            break
    return total_reward
```

## 第32页第10行

Bellmn期望方程为

### 改为

Bellman期望方程为

## 第33页第0行

Bellan期望方程：

### 改为

Bellman期望方程：

## 第33页代码清单2-6

### 题注改为

用Bellman方程求解状态价值和动作价值

### 代码改为

```
def evaluate_bellman(env, policy, gamma=1.):
    a, b = np.eye(env.nS), np.zeros((env.nS))
    for state in range(env.nS - 1):
        for action in range(env.nA):
            pi = policy[state][action]
            for p, next_state, reward, done in env.P[state][action]:
                a[state, next_state] -= (pi * gamma * p)
                b[state] += (pi * reward * p)
    v = np.linalg.solve(a, b)
    q = np.zeros((env.nS, env.nA))
    for state in range(env.nS - 1):
        for action in range(env.nA):
            for p, next_state, reward, done in env.P[state][action]:
                q[state][action] += ((reward + gamma * v[next_state]) * p)
    return v, q
```

## 第34页第2行

$v\left(\sfs\right)-\gamma\sum\limits_{\sfs',r}{p\left(\sfs',r|\sfs,\sfa\right)v_*\left(\sfs'\right)}\ge\sum\limits_{\sfs',r}{rp\left(\sfs',r|\sfs,\sfa\right)}\qquad\sfs\in\mathcal{S}$

### 改为

$v\left(\sfs\right)-\gamma\sum\limits_{\sfs',r}{p\left(\sfs',r|\sfs,\sfa\right)v\left(\sfs'\right)}\ge\sum\limits_{\sfs',r}{rp\left(\sfs',r|\sfs,\sfa\right)}\qquad\sfs\in\mathcal{S},\sfa\in\mathcal{A}$


## 第34页第4页

计算这个动态规划问题

### 改为

计算这个线性规划问题


## 第35页第25行

可以应用下列线性规划求解最优动作价值

### 改为

可以应用下列线性规划求解最优状态价值

## 第36页第2行

$v\left(\sfs\right)\ge r\left(\sfs,\sfa\right)+\gamma\sum\limits_{\sfs'}{p\left(\sfs'|\sfs,\sfa\right){v_*}\left(\sfs'\right)} ,\quad \sfs\in{\cal S}$

### 改为

$v\left(\sfs\right)\ge r\left(\sfs,\sfa\right)+\gamma \sum\limits_{\sfs'}{p\left(\sfs'|\sfs,\sfa\right){v}\left(\sfs'\right)},\quad\sfs\in\mathcal{S},\sfa\in \mathcal{A}$

## 第38页第11行和第14行

这里的$d$都是指$d_\infty$（共3处）。

## 第38页第21行

$d\left(t\left(\sfx'\right),t\left(\sfx''\right)\right)<d\left(\sfx',\sfx''\right)$

### 改为

$d\left(t\left(\sfx'\right),t\left(\sfx''\right)\right)<\gamma d\left(\sfx',\sfx''\right)$

## 第38页第24~25行

用动作价值表示动作价值的形式

### 改为

用状态价值表示状态价值的形式

## 第39页第0行

$v_{*}\left(\sfs'\right)$

### 改为

$v\left(\sfs'\right)$

## 第39页第5行~第7行

$\left|t_\pi\left(v'\right)\left(\sfs\right)-t_\pi\left(v''\right)\left(\sfs\right)\right|\le\gamma\sum\limits_\sfa{\pi\left(\sfa|\sfs\right)\sum\limits_{\sfs'}{p\left(\sfs'|\sfs,\sfa\right)}}\mathop{\max}\limits_{\sfs'} \left|v'\left(\sfs'\right)-v''\left(\sfs'\right)\right|=\gamma d\left(v',v''\right)$

考虑到$v',v''$是任取的，所以有

$d\left(t_\pi\left(v'\right),t_\pi\left(v''\right)\right)\le\gamma d\left(v',v''\right)$

### 改为

$\left|t_\pi\left(v'\right)\left(\sfs\right)-t_\pi\left(v''\right)\left(\sfs\right)\right|\le\gamma\sum\limits_\sfa{\pi\left(\sfa|\sfs\right)\sum\limits_{\sfs'}{p\left(\sfs'|\sfs,\sfa\right)}}\mathop{\max}\limits_{\sfs'}\left|v'\left(\sfs'\right)-v''\left(\sfs'\right)\right|=\gamma d_\infty\left(v',v''\right)$

考虑到$\sfs$是任取的，所以有

$d_\infty\left(t_\pi\left(v'\right),t_\pi\left(v''\right)\right)\le\gamma d_\infty\left(v',v''\right)$

## 第39页第12行~第13行

$\le\max_\sfa\left|f'\left(\sfa'\right)-f''\left(\sfa'\right)\right|$

### 改为（共2处）

$\le\max_\sfa\left|f'\left(\sfa\right)-f''\left(\sfa\right)\right|$

## 第39页第15行~第19行

${t_*}\left(v'\right)\left(\sfs\right)-{t_*}\left(v''\right)\left(\sfs\right)$

$=\mathop{\max}\limits_{\sfa\in\mathcal{A}}\left[r\left(\sfs,\sfa\right)+\gamma\sum\limits_{\sfs'\in{\mathcal{S}}}{p\left(\sfs'\mid\sfs,\sfa\right)v'\left(\sfs'\right)}\right]-\mathop{\max}\limits_{\sfa\in{\mathcal{A}}}\left[r\left(\sfs,\sfa\right)+\gamma\sum\limits_{\sfs'\in{\mathcal{S}}}p\left(\sfs'\mid\sfs,\sfa\right)v''\left(\sfs'\right)\right]$

$\le\mathop{\max}\limits_{\sfa\in\mathcal{A}}\left|\gamma\sum\limits_{\sfs'\in{\mathcal{S}}}{p\left(\sfs'\mid\sfs,\sfa\right)\left(v'\left(\sfs'\right)-v''\left(\sfs'\right)\right)}\right|$

$\le\gamma\left|v'\left(\sfs'\right)-v''\left(\sfs'\right)\right|,$

进而易知$\left|t_*\left(v'\right)\left(\sfs\right)-{t_*}\left(v''\right)\left(\sfs\right)\right| \le\gamma\left|v'\left(\sfs'\right)- v''\left(\sfs'\right)\right|\le\gamma d\left(v',v''\right)$，所以$t_*$是压缩映射。

### 改为

${t_*}\left(v'\right)\left(\sfs\right)-{t_*}\left(v''\right)\left(\sfs\right)$

$=\mathop{\max}\limits_{\sfa\in\mathcal{A}}\left[r\left(\sfs,\sfa\right)+\gamma\sum\limits_{\sfs'\in\mathcal{S}}{p\left(\sfs'\mid\sfs,\sfa\right)v'\left(\sfs'\right)}\right]-\mathop{\max}\limits_{\sfa\in\mathcal{A}}\left[r\left(\sfs,\sfa\right)+\gamma\sum\limits_{\sfs'\in\mathcal{S}}p\left(\sfs'\mid\sfs,\sfa\right)v''\left(\sfs'\right)\right]$

$\le\mathop{\max}\limits_{\sfa\in\mathcal{A}}\left|{\gamma\sum\limits_{\sfs'\in{\mathcal{S}}}{p\left(\sfs'\mid\sfs,\sfa\right)\left(v'\left(\sfs'\right)-v''\left(\sfs'\right)\right)}}\right|$

$\le\gamma\mathop{\max}\limits_{\sfa\in\mathcal{A}}\left|{\sum\limits_{\sfs'\in\mathcal{S}}{p\left(\sfs'|\sfs,\sfa\right)}}\right|\mathop{\max}\limits_{\sfs'\in\mathcal{S}}\left|v'\left(\sfs'\right)-v''\left(\sfs'\right)\right|$

$\le\gamma{d}_\infty\left(v',v''\right)$

进而易知$\left|{{t_*}\left(v'\right)\left(\sfs\right)-{t_*}\left(v''\right)\left(\sfs\right)}\right|\le\gamma d_\infty\left(v',v''\right)$。所以$t_*$是压缩映射。


## 第39页第21行

如果$x\in\mathcal{X}$使得$t\left(x\right)=x$，则称$x$是

### 改为

如果$\sfx\in\mathcal{X}$使得$t\left(\sfx\right)=\sfx$，则称$\sfx$是

## 第39页第22行

（fix point）

### 改为

（fixed point）


## 第42页第7行

对于两个确定性的策略

### 改为

对于策略


## 第42页第8行

$v_\pi\left(\sfs\right)\le{q}_\pi\left(\sfs,\pi'\left( \sfs\right)\right),\quad\sfs\in\mathcal{S}$

### 改为

$v_\pi\left(\sfs\right)\le\sum\limits_\sfa{\pi'\left(\sfa|\sfs\right)q_\pi\left(\sfs,\sfa\right)},\quad\sfs\in\mathcal{S}$

作者注：这里的修改使得策略改进定理不仅可以用于确定性策略的改进。这样修改后，相关的结论就可以用于后续章节中（如第60~61页的4.1.3节）。


## 第42页第12行~第43页第11行

### 证明更新如下

不等式（3-1）等价于

$v_\pi\left(\sfs\right)=\E_{\pi'}\left[v_\pi\left(\sfS_t\right)\mid\sfS_t=\sfs\right]\le\E_{\pi'}\left[q_\pi\left(\sfS_t,\sfA_t\right)\mid\sfS_t=\sfs\right],\quad\sfs\in\mathcal{S}$

其中的期望是针对用策略$\pi'$生成的轨迹中，选取$\sfS_t=\sfs$的那些轨迹而言的。进而有

$\E_{\pi'}\left[v_\pi\left(\sfS_{t+\tau}\right)|\sfS_t=\sfs\right]=\E_{\pi'}\left[\E_{\pi'}\left[v_\pi\left(\sfS_{t+\tau}\right)|\sfS_{t+\tau}\right]|\sfS_t=\sfs\right]\le\E_{\pi'}\left[\E_{\pi'}\left[q_\pi\left(\sfS_{t+\tau},\sfA_{t+\tau}\right)|\sfS_{t+\tau}\right]|\sfS_t=\sfs\right]=\E_{\pi'}\left[q_\pi\left(\sfS_{t+\tau},\sfA_{t+\tau}\right)|\sfS_t=\sfs\right],\quad\sfs\in\mathcal{S},\tau=0,1,2,\ldots$

考虑到

$\E_{\pi'}\left[q_\pi\left(\sfS_{t+\tau},\sfA_{t+\tau}\right)|\sfS_t=\sfs\right]=\E_{\pi'}\left[R_{t+\tau+1}+\gamma{v_\pi}\left(\sfS_{t+\tau+1}\right)|\sfS_t=\sfs\right],\quad\sfs\in\mathcal{S},\tau=0,1,2,\ldots$

所以

$\E_{\pi'}\left[v_\pi\left(\sfS_{t+\tau}\right)|\sfS_t=\sfs\right]\le\E_{\pi'}\left[R_{t+\tau+1}+\gamma{v_\pi}\left(\sfS_{t+\tau+1}\right)|\sfS_t=\sfs\right],\quad\sfs\in\mathcal{S},\tau=0,1,2,\ldots$

进而有

$v_\pi\left(\sfs\right)=\E_{\pi'}\left[v_\pi\left(\sfS_t\right)|\sfS_t=\sfs\right]$

$\le\E_{\pi'}\left[R_{t+1}+\gamma{v_\pi}\left(\sfS_{t+1}\right)|\sfS_t=\sfs\right]$

$\le\E_{\pi'}\left[R_{t+1}+\gamma\E_{\pi'}\left[R_{t+2}+\gamma{v_\pi}\left(\sfS_{t+2}\right)|\sfS_t=\sfs\right]|\sfS_t=\sfs\right]$

$=\E_{\pi'}\left[R_{t+1}+\gamma{R}_{t+2}+\gamma^2v_\pi\left(\sfS_{t+2}\right)\mid\sfS_t=\sfs\right]$

$\le\E_{\pi'}\left[R_{t+1}+\gamma{R}_{t+2}+\gamma^2R_{t+3}+\gamma^3v_\pi\left(\sfS_{t+3}\right)|\sfS_t=\sfs\right]$

$\cdots$

$\le\E_{\pi'}\left[R_{t+1}+\gamma{R}_{t+2}+\gamma^2{R_{t+3}}+\gamma^3R_{t+4}+\cdots|{\sfS_t}=\sfs\right]$

$=\E_{\pi'}\left[G_t|\sfS_t=\sfs\right]$

$=v_{\pi'}\left(\sfs\right)$

## 第48页3.5节

作者注：Gym 0.19版本将FrozenLake-v0改为FrozenLake-v1。

作者注：Gym 0.22删除了DiscreteEnv类，所以env.unwrapped.nS和env.unwrapped.nA不再可用。建议用env.observation_space.n替换env.unwrapped.nA，用env.action_space.n替换env.unwrapped.nA。

## 第49页第12行

这个函数使用`theta`作

### 改为

这个函数使用`tolerant`作

## 第52页代码清单3-10

### 代码改为

```
policy_vi, v_vi = iterate_value(env)
print('状态价值函数 =')
print(v_vi.reshape(4, 4))
print('最优策略 =')
print(np.argmax(policy_vi, axis=1).reshape(4, 4))
episode_rewards = [play_policy(env, policy_vi) for _ in range(100)]
print("价值迭代 平均奖励：{}".format(np.mean(episode_rewards)))
```

## 第54页第15行

$\frac{1}{c}\sum\nolimits_{i=1}^c{g_c}$


### 改为

$\frac{1}{c}\sum\nolimits_{i=1}^c{g_i}$


## 第56页第10行

同时满足$\left\{\alpha_k:k=1,2,\ldots\right\}$

### 改为

$\left\{\alpha_k:k=1,2,\ldots\right\}$同时满足

## 第57页算法4-2第1步，第58页算法4-4第1步（共2处）

初始化动作价值估计

### 改为

初始化状态价值估计

## 第58页最后一行

态$\sfs_{起始}$

#### 改为

态$\sfs_{开始}$

## 第60页第17~20行

证明：对于某个$\varepsilon$柔性策略，用

$\pi\left(\sfa\mid\sfs\right)=\left\{\begin{matrix}1-\varepsilon+\frac{\varepsilon}{\left|\mathcal{A}\right|},&\sfa=\arg\max_{\sfa'}q_\pi\left(\sfs,\sfa'\right)\\ \frac{\varepsilon}{\left|\mathcal{A}\right|},&\sfa\ne\arg\max_\sfa'q_\pi\left(\sfs,\sfa'\right)\end{matrix}\right.$

改进后的策略为$\pi'$，则有

${q_\pi}\left(\sfs,\sfa\right)=\sum\limits_\sfa{\pi'\left(\sfa|\sfs\right){q_\pi}\left(\sfs,\sfa\right)}=\frac{\varepsilon}{{\left|{{\mathcal{A}}\left(\sfs\right)}\right|}}\sum\limits_\sfa{{q_\pi }\left(\sfs,\sfa\right)}+\left(1-\varepsilon\right)\mathop{\max }\limits_\sfa{q_\pi}\left(\sfs,\sfa\right)$

### 改为

证明：考虑在$\varepsilon$柔性策略$\pi$上进行如下方式改进得到的$\varepsilon$柔性策略$\pi'$：

$\pi'\left(\sfa|\sfs\right)=\left\{\begin{matrix}1-\varepsilon+\frac{\varepsilon}{\left|\mathcal{A}\right|},&\sfa=\arg\max_{\sfa'}q_\pi\left(\sfs,\sfa'\right)\\ \frac{\varepsilon}{\left|\mathcal{A}\right|},&\sfa\ne\arg\max_\sfa'q_\pi\left(\sfs,\sfa'\right)\end{matrix}\right.$

在3.2.2节策略改进定理的证明过程中，我们已经证明，只要

$\sum\limits_\sfa{\pi'\left(\sfa|\sfs\right){q_\pi}\left(\sfs,\sfa\right)}\ge{v_\pi}\left(\sfs\right),\quad\sfs\in\mathcal{S}$

就有$\pi\le\pi'$。接下来验证上述不等式。考虑到

$\sum\limits_\sfa\pi'\left(\sfa\mid\sfs\right)q_\pi\left(\sfs,\sfa\right)=\frac{\varepsilon}{\left|\mathcal{A}\left(\sfs\right)\right|}\sum\limits_\sfa{q_\pi}\left(\sfs,\sfa\right)+\left(1-\varepsilon\right)\max\limits_\sfa{q_\pi}\left(\sfs,\sfa\right)$


## 第61页第4行

$q_{\pi'}\left(\sfs,\sfa\right)=\frac{\varepsilon}{\left|\mathcal{A}\left(\sfs\right)\right|}\sum\limits_\sfa{q_\pi\left(\sfs,\sfa\right)}+\left(1-\varepsilon\right)\mathop{\max}\limits_{\sfa}q_\pi\left(\sfs,\sfa\right)$

## 改为

$\sum\limits_{\sfa}{\pi'\left(\sfa|\sfs\right)q_\pi\left(\sfs,\sfa\right)}=\frac{\varepsilon}{\left|\mathcal{A}\left(\sfs\right)\right|}\sum\limits_\sfa{q_\pi\left(\sfs,\sfa\right)}+\left(1-\varepsilon\right)\mathop{\max}\limits_{\sfa}q_\pi\left(\sfs,\sfa\right)$


## 第63页第13行

回报是等概率出现的是。

### 改为

回报是等概率出现的。

## 第64页第3行

$v\leftarrow v+\frac{1}{c}\left(g-v\right)$

### 改为

$v\leftarrow v+\frac{\rho}{c}\left(g-v\right)$

## 第64页第9行

算法4-7给出了每次方法加权重要性采样

### 改为

算法4-7给出了每次访问加权重要性采样

## 第64页算法4-7第2.4.2步

$w\left[G-q\left(\sfS_t,\sfA_t\right)\right]^2$


### 改为

$\rho\left[G-q\left(\sfS_t,\sfA_t\right)\right]^2$


## 第65页算法4-8第2.2步

$\sfS_0,\sfA_0,R_1,\sfS_1,\sfA_1,R_1,\ldots,\sfS_{T-1},\sfA_{T-1},R_T,\sfS_T$

### 改为

$\sfS_0,\sfA_0,R_1,\sfS_1,\sfA_1,R_2,\ldots,\sfS_{T-1},\sfA_{T-1},R_T,\sfS_T$

## 第66页4.3节

作者注：Gym 0.20版本将Blackjack-v0改为Blackjack-v1。

## 第67页第3行

范围为3~21的int型数值

### 改为

范围为4~21的int型数值


## 第67页第5行

是有将1张A牌计算为11点。

### 改为

是否将1张A牌计算为11点。

## 第71页第7行

最优价值和最优价值函数

### 改为

最优策略和最优价值函数

## 第72页4.3.4节正文第1行，第73页正文第0行

`evaluate_action_monte_carlo_importance_resample`

### 改为

`evaluate_monte_carlo_importance_sample`

## 第72页代码清单4-7第0行，第73页正文第7行

`evaluate_monte_carlo_importance_resample`

### 改为

`evaluate_monte_carlo_importance_sample`

## 第73页代码清单4-8第0行，第74页正文第0行，第74页正文第3行

`monte_carlo_importance_resample`

改为

`monte_carlo_importance_sample`

## 第75页第8行

$\rho_{t:T-1}=\prod\limits_{k=t}^{T-1}{\frac{{\pi\left(\sfA_t|\sfS_t\right)}}{{b\left(\sfA_t|\sfS_t\right)}}}$

### 改为

$\rho_{t:T-1}=\prod\limits_{\tau=t}^{T-1}{\frac{\pi\left(\sfA_\tau|\sfS_\tau\right)}{{b\left(\sfA_\tau|\sfS_\tau\right)}}}$


## 第77页第2行

$U_{t:t+1}^{\left(q\right)}=R_{t+1}+\gamma q\left(\sfS_{t+1},\sfA_{t+n}\right)$

### 改为

$U_{t:t+1}^{\left(q\right)}=R_{t+1}+\gamma q\left(\sfS_{t+1},\sfA_{t+1}\right)$


## 第77页第15行

$U_{t:t+1}^{\left(q\right)}=\left\{\begin{matrix}{R_{t+1}+\gamma{R}_{t+2}+\cdots+\gamma^{n-1}{R}_{t+n}+{\gamma^n}q\left(\sfS_{t+1},\sfA_{t+n}\right),}&t+n<T,\\R_{t+1}+\gamma{R}_{t+2}+\cdots+\gamma^{T-t-1}R_T,&t+n\ge{T}.\end{matrix}\right.$

### 改为

$U_{t:t+n}^{\left(q\right)}=\left\{\begin{matrix}{R_{t+1}+\gamma{R}_{t+2}+\cdots+\gamma^{n-1}{R}_{t+n}+{\gamma^n}q\left(\sfS_{t+n},\sfA_{t+n}\right),}&t+n<T,\\R_{t+1}+\gamma{R}_{t+2}+\cdots+\gamma^{T-t-1}R_T,&t+n\ge{T}.\end{matrix}\right.$

## 第79页算法5-2第2.1步

2.1（初始化状态动作对）

### 改为
2.1（初始化状态）

## 第81页算法5-4输出

输出：动作价值估计

### 改为

输出：状态价值估计

## 第81页算法5-4第2.2.1步

$U\leftarrow{R}_{t+1}+\gamma{R}_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+{\gamma^n}v\left(\sfS_{t+n},{\gamma^n}v\sfS_{t+n}\right)$


### 改为

$U\leftarrow{R}_{t+1}+\gamma{R}_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+{\gamma^n}v\left(\sfS_{t+n}\right)$


## 第84页算法5-8步骤2.2

前一个步骤2.2的编号应为2.1。

## 第85页第11行

$=\prod\limits_{\tau=t}^{t+n-1}\frac{\pi\left(\sfA_\tau|\sfS_\tau\right)}{b\left(\sfA_\tau|\sfS_\tau\right)}$


### 改为

$=\prod\limits_{\tau=t+1}^{t+n-1}\frac{\pi\left(\sfA_\tau|\sfS_\tau\right)}{b\left(\sfA_\tau|\sfS_\tau\right)}$

## 第85页算法5-10输入

输入：环境（无数学描述）、策略$\pi$。

#### 改为

输入：环境（无数学描述）、如果是策略评估还需要输入策略$\pi$。


## 第85页算法5-10输出

若是最优策略控制则还是输出策略

### 改为

若是最优策略控制则还要输出策略

## 第85页算法5-10第2.3.1步

根据$\pi\left(\cdot\mid\sfS_{t+n}\right)$决定

#### 改为

根据$b\left(\cdot\mid\sfS_{t+n}\right)$决定

## 第87页算法5-11第2.1步

初始化状态动作对

### 改为

初始化状态

## 第87页第14行

使得从$\sfs$<sub>状态</sub>更倾向于选择

### 改为

使得从$\sfs$<sub>开始</sub>更倾向于选择

## 第88页算法5-12第2.1步

2.1（初始化状态动作对）

### 改为

2.1（初始化状态）

## 第88页算法5-12第2.2.4步

$U\leftarrow{R}+\gamma{q^{\left(1-i\right)}}\left(\sfS_{t+1},\arg{{\max}_\sfa}{q^{\left(i\right)}}\left(\sfS_{t+1},\sfa\right)\right)$


### 改为

$U\leftarrow{R}+\gamma{q^{\left(1-i\right)}}\left(\sfS',\arg{\max_\sfa}{q^{\left(i\right)}}\left(\sfS',\sfa\right)\right)$


## 第89页图5-3子图a和子图b（共2处）

$\left(1-\lambda\right)\lambda^{T-t-1}$

### 改为

$\lambda^{T-t-1}$


## 第90页第8行

时序差分目标$U_{t-n:t}$的权重是$\left(1-\lambda\right)\lambda^n$。


### 改为

时序差分目标$U_{t-n:t}$的权重是$\left(1-\lambda\right)\lambda^{n-1}$。


## 第90页第13行

对的单步自益结果

### 改为

的单步自益结果

## 第90页第20行

$U_{\tau:t}=R_{\tau+1}+\cdots+\gamma^{t-\tau-1}U_{t:t+1}$，所以$U_{t:t+1}$

### 改为

$U_{\tau:t}=R_{\tau+1}+\cdots+\gamma^{t-\tau-1}U_{t-1:t}$，所以$U_{t-1:t}$

## 第91页算法5-13参数部分、第92页算法5-14参数部分

参数：资格迹参数$\beta$、

#### 改为

参数：资格迹参数$\lambda$和$\beta$、

## 第91页算法5-13

### 将第1步的

初始化资格迹$e\left(\sfs,\sfa\right)\leftarrow{0},\sfs\in\mathcal{S},\sfa\in\mathcal{A}\left(\sfs\right)$。

### 移动到2.1步


## 第91页算法5-13第2.2.2步~2.2.5步

### 改为

2.2.2 根据输入策略$\pi\left(\cdot|\sfS'\right)$或是迭代的最优价值函数$q\left(\sfS',\cdot\right)$确定动作$\sfA'$；

2.2.3（更新资格迹）$e\left(\sfs,\sfa\right)\leftarrow\gamma\lambda{e}\left(\sfs,\sfa\right),\sfs\in\mathcal{S},\sfa\in\mathcal{A}\left(\sfs\right)$，$e\left(\sfS,\sfA\right)\leftarrow{1}+\beta{e}\left(\sfS,\sfA\right)$；

2.2.4（计算回报的估计值）$U\leftarrow{R}+\gamma{q}\left(\sfS',\sfA'\right)$；

2.2.5（更新价值）$q\left(\sfs,\sfa\right)\leftarrow{q}\left(\sfs,\sfa\right)+\alpha{e}\left(\sfs,\sfa\right)\left[U-q\left(\sfS,\sfA\right)\right],\sfs\in\mathcal{S},\sfa\in\mathcal{A}\left(\sfs\right)$;


## 第92页算法5-14第1步

### 删去

初始化资格迹$c\left(\sfs\right)\leftarrow{0},\sfs\in\mathcal{S}$。


## 第92页算法5-14第2.1步

2.1（初始化状态动作对）选择状态$v\left(\sfs\right)$。

### 改为

2.1（初始化状态和资格迹）选择状态$\sfS$，初始化资格迹$e\left(\sfs\right)\leftarrow{0},\sfs\in\mathcal{S}$。


## 第92页算法5-14第2.3.3步

$\sfs\in\mathcal{S},\sfa\in\mathcal{A}$

### 改为

$\sfs\in\mathcal{S}$


## 第92页算法5-14第2.2.5步

### 改为

2.2.5（更新价值）$v\left(\sfs\right)\leftarrow{v}\left(\sfs\right)+\alpha{e}\left(\sfs\right)\left[U-v\left(\sfS\right)\right],\sfs\in\mathcal{S}$。


## 第92页第11行、第93页第12行、第94页代码清单5-1第1行

Taxi-v2

### 改为

Taxi-v3

（共3处。Gym 0.15.2版本Taxi-v2变更地图为Taxi-v3）

## 第93行图5-5

```
+---------+
|R: | : :G|
| : : : : |
| : : : : |
| | : | : |
|Y| : |B: |
+---------+
```

### 改为

```
+---------+
|R: | : :G|
| : | : : |
| : : : : |
| | : | : |
|Y| : |B: |
+---------+
```

（Gym 0.15.2版本Taxi-v2变更地图为Taxi-v3）

## 第96页代码清单5-6第14行

```
        v = (self.q[next_state].sum() * self.epsilon + \
```

### 改为

```
        v = (self.q[next_state].mean() * self.epsilon + \
```

## 第98页第2行

`DoubleQLearnignAgent`类

### 改为

`DoubleQLearningAgent`类

## 第99页代码清单5-12

### 改为

```
class SARSALambdaAgent(SARSAAgent):
    def __init__(self, env, lambd=0.5, beta=1.,
            gamma=0.9, learning_rate=0.1, epsilon=.01):
        super().__init__(env, gamma=gamma, learning_rate=learning_rate,
                epsilon=epsilon)
        self.lambd = lambd
        self.beta = beta
        self.e = np.zeros((env.observation_space.n, env.action_space.n))
        
    def learn(self, state, action, reward, next_state, done, next_action):
        # 更新资格迹
        self.e *= (self.lambd * self.gamma)
        self.e[state, action] = 1. + self.beta * self.e[state, action]
        
        # 更新价值
        u = reward + self.gamma * \
                self.q[next_state, next_action] * (1. - done)
        td_error = u - self.q[state, action]
        self.q += self.learning_rate * self.e * td_error
        
        # 为下一回合初始化资格迹
        if done:
            self.e *= 0.

agent = SARSALambdaAgent(env)
```

## 第100页第12行

$U_t^{\left(i\right)}=R_{t+1}+\gamma{q}^{\left(i\right)}\left(\sfS_{t+1},\arg\max_{\sfa}q^{\left(1-i\right)}\left(\sfS_{t+1},\sfa\right)\right)$


### 改为

$U_t^{\left(i\right)}=R_{t+1}+\gamma{q}^{\left(1-i\right)}\left(\sfS_{t+1},\arg\max_{\sfa}q^{\left(i\right)}\left(\sfS_{t+1},\sfa\right)\right)$


## 第100页第16行

形式为$U_t=R_{t+1}+q\left(\sfS_{t+1},\sfA_{t+1}\right)$。


### 改为

形式为$U_t=R_{t+1}+\gamma{q}\left(\sfS_{t+1},\sfA_{t+1}\right)$。

## 第102页第5行

回合的损失为$\sum\nolimits_{t=0}^T{{\left[G_t-q\left(\sfS_t,\sfA_t;\mathbf{w}\right)\right]}^2}$。如果我们沿着$\sum\nolimits_{t=0}^T{{\left[G_t-q\left(\sfS_t,\sfA_t;\mathbf{w}\right)\right]}^2}$


### 改为

回合的损失为$\sum\nolimits_{t=0}^{T-1}{{\left[G_t-q\left(\sfS_t,\sfA_t;\mathbf{w}\right)\right]}^2}$。如果我们沿着$\sum\nolimits_{t=0}^{T-1}{{\left[G_t-q\left(\sfS_t,\sfA_t;\mathbf{w}\right)\right]}^2}$


## 第102页第12行

$\sum\nolimits_{t=0}^T{{\left[G_t-v\left(\sfS_t;\mathbf{w}\right)\right]}^2}$

### 改为

$\sum\nolimits_{t=0}^{T-1}{{\left[G_t-v\left(\sfS_t;\mathbf{w}\right)\right]}^2}$


## 第103页第5行

$\sum\nolimits_{t=0}^T{{\left[U_t-q\left(\sfS_t,\sfA_t;\mathbf{w}\right)\right]}^2}$


### 改为

$\sum\nolimits_{t=0}^{T-1}{{\left[U_t-q\left(\sfS_t,\sfA_t;\mathbf{w}\right)\right]}^2}$


## 第104页算法6-3第2.2.4步

$\mathbf{w}\leftarrow\mathbf{w}+\alpha\left[G-q\left(\sfS,\sfA;\mathbf{w}\right)\right]\nabla{q}\left(\sfS,\sfA;\mathbf{w}\right)$


### 改为

$\mathbf{w}\leftarrow\mathbf{w}+\alpha\left[U-q\left(\sfS,\sfA;\mathbf{w}\right)\right]\nabla{q}\left(\sfS,\sfA;\mathbf{w}\right)$


## 第104页算法6-4第2.1步

2.1（初始化状态动作对）选择状态$\sfS$，再根据输入策略$\pi$选择动作$\sfA$。

### 改为

2.1（初始化状态）选择状态$\sfS$。


## 第104页算法6-4第2.2.3步、第106页算法6-7第2.2.3步（共2处）

删去此步

## 第104页算法6-4第2.2.4步、第106页算法6-7第2.2.4步（共2处）

如果是动作价值评估

### 改为

如果是状态价值评估


## 第104页算法6-4第2.2.5步，第106页算法6-7第2.2.6步（共2处）

（更新动作价值函数）

### 改为

（更新价值函数）


## 第104页第5行

算法6-5

### 改为

算法6-3

## 第105页算法6-5

删去该算法

## 第105页第9~10行

$\mathbf{w}\leftarrow{\mathbf{w}}+\alpha\left[U-q\left(\sfS_t,\sfA_t\right)\right]\mathbf{z}$, 更新动作价值

$\mathbf{w}\leftarrow{\mathbf{w}}+\alpha\left[U-v\left(\sfS_t\right)\right]\mathbf{z}$, 更新状态价值


### 改为

$\mathbf{w}\leftarrow\mathbf{w}+\alpha\left[U-q\left(\sfS_t,\sfA_t;\mathbf{w}\right)\right]\mathbf{z}$, 更新动作价值

$\mathbf{w}\leftarrow\mathbf{w}+\alpha\left[U-v\left(\sfS_t;\mathbf{w}\right)\right]\mathbf{z}$, 更新状态价值

## 第105页算法6-5第2.1步

#### 增加

初始化资格迹$\mathbf{z}\leftarrow\mathbf{0}$

## 第106页算法6-7第2.1步

2.1（初始化状态动作对）选择状态$\sfS$，再根据输入策略$\pi$选择动作$\sfA$。

### 改为

2.1（初始化状态和资格迹）选择状态$\sfS$，初始化资格迹$\mathbf{z}\leftarrow\mathbf{0}$。


## 第109页算法6-8输出和第1步

$q\left(\sfs_t,\sfa_t;\mathbf{w}\right)$


### 改为

$q\left(\sfs,\sfa;\mathbf{w}\right)$


## 第109页算法6-8第2.2步

决定确定性策略

### 改为

决定策略

## 第111页算法6-9第2.1步、第113页算法6-10第2.1步（共2处）

2.1（初始化状态动作对）

### 改为

2.1（初始化状态）

## 第112页第12行

$P_i=\frac{p_i}{\sum_{k}{p_k}}$


### 改为

$\frac{p_i}{\sum_{k}{p_k}}$


## 第112页第14行

其中$\delta_i$是时序差分误差，

### 改为

其中$\delta_i$是时序差分误差（定义为$\delta_t=U_t-q\left(\sfS_t,\sfA_t;\mathbf{w}\right)$或$\delta_t=U_t-v\left(\sfS_t;\mathbf{w}\right)$），

## 第114页6.4.4节

本节将“duel network”译作“对偶网络”有误。可译为“决斗网络”。相应的“对偶Q网络”、“对偶深度Q网络”可改为“决斗Q网络”、“决斗深度Q网络”。


## 第118页

作者注：

砖瓦编码是一种历史悠久的特征构造方法，可用于回归、分类等问题。目前学术界倾向于用神经网络代替砖瓦编码来构造特征。由于砖瓦编码和强化学习没有直接关联，本书没有用过多的篇幅介绍砖瓦编码。

实际使用砖瓦编码时，不需要精确计算砖瓦的数量，常随意的大致估计砖瓦的数量作为特征数。如果设置的特征数大于真实的砖瓦数量，那么有些特征永远不会取到，有些浪费；如果设置的特征数小于真实的砖瓦数量，那么有多个砖瓦需要共享特征，具体逻辑可以见代码清单6-3中“冲突处理”部分。这些浪费和冲突往往不会造成明显的性能损失。

第118页砖瓦数计算：每个大网格的网格宽度刚好是整个取值范围的1/8。所以，第0层大网格每个维度有8个大网格；第1~7层由于有偏移，每个维度需要有9个大网格才能覆盖整个取值范围。第117页图6-3b的情况略有不同：这个图中每个维度的取值范围不是大网格的长度的整数倍。所以有些层偏移后，不需要更多的大网格也可以覆盖整个取值范围。


## 第126页第10行

再配合其他一些容易活动的

### 改为

再配合其他一些容易获得的


## 第126页最后一行

$=\sum\limits_\sfs\Pr\left[\sfS_t=\sfs\right]\nabla{v}_{\pi\left(\bftheta\right)}\left(\sfS_t\right)$

### 改为

$=\sum\limits_\sfs\Pr\left[\sfS_t=\sfs\right]\nabla{v}_{\pi\left(\bftheta\right)}\left(\sfs\right)$

## 第127页第10行

$=\E\left[\sum\limits_\sfa{q}_{\pi\left(\bftheta\right)}\left(\sfS_0,\sfa\right)\nabla\pi\left(\sfa|\sfS_0;\bftheta\right)\right]+\E\left[\sum\limits_\sfa\gamma{q}_{\pi\left(\bftheta\right)}\left(\sfS_1,\sfa\right)\nabla\pi\left(\sfa|\sfS_1;\bftheta\right)\right]+\gamma^2\E\left[\nabla{v}_{\pi\left(\bftheta\right)}\left(\sfS_1\right)\right]$

改为

$=\E\left[\sum\limits_\sfa{q}_{\pi\left(\bftheta\right)}\left(\sfS_0,\sfa\right)\nabla\pi\left(\sfa|\sfS_0;\bftheta\right)\right]+\E\left[\sum\limits_\sfa\gamma{q}_{\pi\left(\bftheta\right)}\left(\sfS_1,\sfa\right)\nabla\pi\left(\sfa|\sfS_1;\bftheta\right)\right]+\gamma^2\E\left[\nabla{v}_{\pi\left(\bftheta\right)}\left(\sfS_2\right)\right]$


## 第127页第18行

$=\E\left[\gamma^t{q}_{\pi\left(\bftheta\right)}\left(\sfS_t,\sfA_t\right)\nabla\pi\left(\sfA_t|\sfS_t;\bftheta\right)\right]$


### 改为

$=\E\left[\gamma^t{q}_{\pi\left(\bftheta\right)}\left(\sfS_t,\sfA_t\right)\nabla\ln\pi\left(\sfA_t|\sfS_t;\bftheta\right)\right]$


## 第127页第20行

$\E\left[\sum\limits_\sfa{\gamma^t}q_{\pi\left(\bftheta\right)}\left(\sfS_t,\sfa\right)\nabla\pi\left(\sfa|\sfS_t;\bftheta\right)\right]=\E\left[\gamma^t{q}_{\pi\left(\bftheta\right)}\left(\sfS_t,\sfA_t\right)\nabla\pi\left(\sfA_t|\sfS_t;\bftheta\right)\right]=\E\left[\gamma^t{G_t}\nabla\pi\left(\sfA_t|\sfS_t;\bftheta\right)\right]$


### 改为

$\E\left[\sum\limits_\sfa{\gamma^t}q_{\pi\left(\bftheta\right)}\left(\sfS_t,\sfa\right)\nabla\pi\left(\sfa|\sfS_t;\bftheta\right)\right]=\E\left[\gamma^t{q}_{\pi\left(\bftheta\right)}\left(\sfS_t,\sfA_t\right)\nabla\ln\pi\left(\sfA_t|\sfS_t;\bftheta\right)\right]=\E\left[\gamma^t{G_t}\nabla\ln\pi\left(\sfA_t|\sfS_t;\bftheta\right)\right]$


## 第128页第10行

增量$\alpha\gamma^tG_t\nabla\pi\left(\sfA_t|\sfS_t;\bftheta_t\right)$

#### 改为

增量$\alpha\gamma^tG_t\nabla\pi\left(\sfA_t|\sfS_t;\bftheta\right)$


## 第128页算法7-1输入和第130页算法7-2输入（共2处）

输入：环境（无数学描述）、策略$\pi$。

### 改为

输入：环境（无数学描述）。

## 第128页算法7-1第2步和第130页算法7-2第2步（共2处）

2.（时序差分更新）

### 改为

2.（回合更新）


## 第129页第10~11行

$=\sum\limits_\sfa{\gamma^t\left(G_t-B\left(\sfS_t\right)\right)\nabla\pi\left(\sfA_t|\sfS_t;\bftheta\right)}$

$=\sum\limits_\sfa{\gamma^tG_t\nabla\pi\left(\sfA_t|\sfS_t;\bftheta\right)}$


### 改为

$=\sum\limits_\sfa{\gamma^t\left(G_t-B\left(\sfS_t\right)\right)\nabla\pi\left(\sfa|\sfS_t;\bftheta\right)}$

$=\sum\limits_\sfa{\gamma^tG_t\nabla\pi\left(\sfa|\sfS_t;\bftheta\right)}$


### 第129页第15行

随机变量$B\left(\sfS\right)=-\sum\limits_{\tau=1}^{t-1}\gamma^{\tau-t}R_\tau$，

### 改为

随机变量$B\left(\sfS\right)=-\sum\limits_{\tau=0}^{t-1}\gamma^{\tau-t}R_{\tau+1}$，


## 第129页第24行

值函数的估计是可以得到的。

### 改为

值函数的估计。

## 第130页算法7-2第2.3.3步

更新$\bftheta$以减小$-\gamma^t\left[G-v\left(\sfS_t;\mathbf{w}\right)\right]\nabla\ln\pi\left(\sfA_t|\sfS_t;\bftheta\right)$


## 改为

更新$\bftheta$以减小$-\gamma^t\left[G-v\left(\sfS_t;\mathbf{w}\right)\right]\ln\pi\left(\sfA_t|\sfS_t;\bftheta\right)$


## 第130页第4行

$\E\left[-2\gamma^t\left(G_t-B\left(\sfS_t\right)\right){\left[ {\nabla\ln\pi\left(\sfA_t|\sfS_t;\bftheta\right)} \right]}^2\right]$


### 改为

$\E\left[-2\gamma^{2t}\left(G_t-B\left(\sfS_t\right)\right){\left[ {\nabla\ln\pi\left(\sfA_t|\sfS_t;\bftheta\right)} \right]}^2\right]$


## 第130页第11行

不过，由于梯度$\nabla\ln\pi\left(\sfA_t|\sfS_t;\bftheta\right)$并不会预先知道，所以实际应用时


### 改为

但是，在实际应用中，无法事先知道这个值，所以

## 第131页算法7-3第2.3步

2.3（初始化回报和权重）

#### 改为

2.3（初始化回报）

## 第139页第17行

减了基线$b\left(\sfs\right)=v_\pi\left(\sfs\right)$

### 改为

减去基线$B\left(\sfs\right)=v_\pi\left(\sfs\right)$


## 第137页代码清单7-1中`learn()`函数

```
            y = np.eye(self.action_n)[df['action']] * \
                    df['psi'].values[:, np.newaxis]
            self.policy_net.fit(x, y, verbose=0)
```

#### 改为

```
            sample_weight = df['psi'].values[:, np.newaxis]
            y = np.eye(self.action_n)[df['action']]
            self.policy_net.fit(x, y, sample_weight=sample_weight, verbose=0)
```


## 第139页第18行

加上基线$b\left(\sfs\right)=v_\pi\left(\sfs\right)$

### 改为

减去基线$B\left(\sfs\right)=v_\pi\left(\sfs\right)$

## 第139页第13行和第18行（2处）

$R_t+\gamma{v_\pi}\left(\sfS_{t+1}\right)$


### 改为

$R_{t+1}+\gamma{v_\pi}\left(\sfS_{t+1}\right)$


## 第140页第4行

$R_{t}+v_\pi\left(\sfS_{t+1};\mathbf{w}\right)$


### 改为

$R_{t+1}+\gamma{v_\pi}\left(\sfS_{t+1};\mathbf{w}\right)$


## 第140页算法8-1第2.2步

2.2（决定初始动作）用$\pi\left(\cdot|\sfS;\bftheta\right)$得到动作$\sfA$；

### 改为

2.2（决定初始状态动作对）选择状态$\sfS$，用$\pi\left(\cdot|\sfS;\bftheta\right)$得到动作$\sfA$；


## 第140页算法8-1第2.3.1步

得到采样$R$和

### 改为

得到奖励$R$和

## 第141页第4行和第5行

$R_t+\gamma{v_\pi}\left(\sfS_{t+1};\mathbf{w}\right)$


### 改为

$R_{t+1}+\gamma{v_\pi}\left(\sfS_{t+1};\mathbf{w}\right)$

## 第141页算法8-2的第2.2步至第2.3.3步，第143页算法8-4的第2.2步至第2.3.3步

2.2（决定初始动作）用$\pi\left(\cdot|\sfS;\bftheta\right)$得到动作$\sfA$；

2.3 如果回合未结束，执行以下操作：

2.3.1（采样）根据状态$\sfS$和动作$\sfA$得到采样$R$和下一状态$\sfS'$；

2.3.2（执行）用$\pi\left(\cdot|\sfS';\bftheta\right)$得到动作$\sfA'$

2.3.3（估计回报）$U\leftarrow{R}+\gamma{q}\left(\sfS',\sfA';\mathbf{w}\right)$


### 改为

2.2（决定初始状态）选择状态$\sfS$；

2.3 如果回合未结束，执行以下操作：

2.3.1（采样）用$\pi\left(\cdot|\sfS;\bftheta\right)$得到动作$\sfA$；

2.3.2（执行）执行动作$\sfA$，得到奖励$R$和观测$\sfS'$；

2.3.3（估计回报）$U\leftarrow{R}+\gamma{v}\left(\sfS';\mathbf{w}\right)$


## 第141页算法8-2的第2.3.7步，第143页算法8-4的第2.3.9步

$\sfS\leftarrow\sfS'$，$\sfA\leftarrow\sfA'$


### 改为

$\sfS\leftarrow\sfS'$


## 第142页算法8.3第2.3.2步

作者注：这里的更新式子遵循了论文原文而没有考虑累积折扣。推导出现折扣是正确的；更新时考虑折扣也是正确和合理的。

## 第143页第5行，第143页算法8.4第2.3.4步（共2处）

$\nabla\ln\pi\left(\sfA\mid\sfS;\mathbf{w}\right)$

改为

$\nabla\ln\pi\left(\sfA\mid\sfS;\bftheta\right)$


## 第143页算法8-4

### 将第1步的

初始化资格迹$\mathbf{z}^{\left(\bftheta \right)}\leftarrow\mathbf{0}$，$\mathbf{z}^{\left(\mathbf{w}\right)}\leftarrow\mathbf{0}$。


### 移动到第2.1步

## 第143页算法8-4第2.2步

2.2（决定初始动作）用$\pi\left(\cdot|\sfS;\bftheta\right)$得到动作$\sfA$；

### 改为

2.2（决定初始状态）选择状态$\sfS$；


## 第144页第5~9行

$\E_{\pi\left(\bftheta\right)}\left[\sum\limits_{t=0}^{+\infty}{\gamma^t{a_{\pi_k}}\left(\sfS_t,\sfA_t\right)}\right]$

$=\E_{\pi\left(\bftheta\right)}\left[\sum\limits_{t=0}^{+\infty}{\gamma^t\left(R_t+\gamma{v_{\pi\left(\bftheta_k\right)}}\left(\sfS_{t+1}\right)-v_{\pi\left(\bftheta_k\right)}\left(\sfS_t\right)\right)}\right]$

$=\E_{\pi\left(\bftheta\right)}\left[-v_{\pi\left(\bftheta_k\right)}\left(\sfS_0\right)+\sum\limits_{t=0}^{+\infty}{\gamma^tR_t}\right]$

$=-\E_{\sfS_0}\left[v_{\pi\left(\bftheta_k\right)}\left(\sfS_0\right)\right]+\E_{\pi\left(\bftheta\right)}\left[\sum\limits_{t=0}^{+\infty}{\gamma^tR_t}\right]$

$=-\E_{\pi\left(\bftheta_k\right)}\left[G_0\right]+\E_{\pi\left(\bftheta\right)}\left[G_0\right].$


### 改为

$\E_{\pi\left(\bftheta\right)}\left[\sum\limits_{t=0}^{+\infty}{\gamma^{t}a_{\pi\left(\bftheta_k\right)}\left(\sfS_t,\sfA_t\right)}\right]$

$=\E_{\pi\left(\bftheta\right)}\left[\sum\limits_{t=0}^{+\infty}{\gamma^t\left(R_{t+1}+\gamma{v}_{\pi\left(\bftheta_k\right)}\left(\sfS_{t+1}\right)-v_{\pi\left(\bftheta_k\right)}\left(\sfS_t\right)\right)}\right]$

$=\E_{\pi\left(\bftheta\right)}\left[-v_{\pi\left(\bftheta_k\right)}\left(\sfS_0\right)+\sum\limits_{t=0}^{+\infty}{\gamma^tR_{t+1}}\right]$

$=-\E_{\sfS_0}\left[v_{\pi\left(\bftheta_k\right)}\left(\sfS_0\right)\right]+\E_{\pi\left(\bftheta\right)}\left[\sum\limits_{t=0}^{+\infty}{\gamma^t{R_{t+1}}}\right]$

$=-\E_{\pi\left(\bftheta_k\right)}\left[G_0\right]+\E_{\pi\left(\bftheta\right)}\left[G_0\right].$

## 第144页第13行和第16行（共2处）

$\E_{\sfS_t,\sfA_t\sim\pi\left(\bftheta\right)}\left[a_{\pi_k}\left(\sfS_t,\sfA_t\right)\right]$

#### 改为

$\E_{\sfS_t,\sfA_t\sim\pi\left(\bftheta\right)}\left[a_{\pi_{\left(\bftheta_k\right)}}\left(\sfS_t,\sfA_t\right)\right]$


## 第145页算法8-5第2.3步

更新$\bftheta$以减小

### 改为

更新$\bftheta$以增大


## 第145页算法8-5第2.4步

最小化$\left[G_t-v\left(\sfS_t,\sfA_t;\mathbf{w}\right)\right]^2$

### 改为

最小化$\left[G_t-v\left(\sfS_t;\mathbf{w}\right)\right]^2$


## 第147页图8-1中间线的注记

$l_{\left(\bftheta\right)}$

### 改为

$l\left(\bftheta\right)$


## 第148页第7行

minimize

### 改为

maximize

## 第149页第7行

如果它们满足$\mathbf{p}_i\mathbf{F}\mathbf{p}_j=\mathbf{0}$


### 改为

如果它们满足$\mathbf{p}_i^\text{T}\mathbf{F}\mathbf{p}_j=0$


## 第150页第5行

$\frac{\partial}{\partial\alpha_k}\left(\frac{1}{2}{{\left(\mathbf{x}_k+\alpha_k\mathbf{p}_k\right)}^\mathrm{T}}\mathbf{F}\left(\mathbf{x}_k+\alpha_k\mathbf{p}_k\right)-\mathbf{g}^{\mathrm{T}}\left(\mathbf{x}_k+\alpha\mathbf{p}_k\right)\right)=\alpha_k\mathbf{p}_k^\mathrm{T}\mathbf{F}\mathbf{p}_k-\mathbf{p}_k^\mathrm{T}\left(\mathbf{F}\mathbf{x}_k-\mathbf{g}\right)$


### 改为

$\frac{\partial}{\partial\alpha_k}\left(\frac{1}{2}{{\left(\mathbf{x}_k+\alpha_k\mathbf{p}_k\right)}^\mathrm{T}}\mathbf{F}\left(\mathbf{x}_k+\alpha_k\mathbf{p}_k\right)-\mathbf{g}^{\mathrm{T}}\left(\mathbf{x}_k+\alpha_k\mathbf{p}_k\right)\right)=\alpha_k\mathbf{p}_k^\mathrm{T}\mathbf{F}\mathbf{p}_k+\mathbf{p}_k^\mathrm{T}\left(\mathbf{F}\mathbf{x}_k-\mathbf{g}\right)$


## 第150页第7行

$\alpha_k=\frac{\mathbf{p}_k^\mathrm{T}\left(\mathbf{F}\mathbf{x}_k-\mathbf{g}\right)}{\mathbf{p}_k^\mathrm{T}\mathbf{F}\mathbf{p}_k}$

### 改为

$\alpha_k=\frac{\mathbf{p}_k^\mathrm{T}\left(\mathbf{g}-\mathbf{F}\mathbf{x}_k\right)}{\mathbf{p}_k^\mathrm{T}\mathbf{F}\mathbf{p}_k}$


## 第150页算法8-7第2.1步

2.1  $\mathbf{z}\leftarrow\mathbf{Fx}$

### 改为

2.1  $\mathbf{z}\leftarrow\mathbf{Fp}$


## 第151页第6行

minimize

### 改为

maximize

## 第154页算法8-10第2.2步

2.2（决定初始动作）用$b\left(\cdot|\sfS\right)$得到动作$\sfA$；

### 改为

2.2（决定初始状态动作对）选择状态$\sfS$，用$b\left(\cdot|\sfS\right)$得到动作$\sfA$；

## 第154页算法8-10第2.3.2步

得到动作$A'$

### 改为

得到动作$\sfA'$

## 第155页第27行

$=-\sum\limits_\sfa{\frac{\pi\left(\sfa|{\sfS_t};\bftheta^\mathrm{EMA}\right)}{\pi\left(\sfa|{\sfS_t};\bftheta\right)}}.$


### 改为

$=-\sum\limits_\sfa\frac{\pi\left(\sfa\mid\sfS_t;\bftheta^\mathrm{EMA}\right)}{\pi\left(\sfa\mid\sfS_t;\bftheta\right)} {\nabla_{\bftheta}} \pi\left(\sfa\mid\sfS_t;\bftheta\right).$

## 第156页第14~16行

将前式代入后式可得$\lambda_t=\frac{\mathbf{k}_t^\textrm{T}\mathbf{g}-\delta}{\mathbf{k}_t^\textrm{T}\mathbf{k}_t}$。由于Lagrange乘子应当大等于0，所以Lagrange乘子应

为$\max\left\{\frac{\mathbf{k}_t^\textrm{T}\mathbf{g}-\delta}{\mathbf{k}_t^\textrm{T}\mathbf{k}_t},0\right\}$，优化问题的最优解为

$\mathbf{z}_t=\mathbf{g}_t-\max\left\{\frac{bf{k}_t^\textrm{T}\mathbf{g}-\delta}{\mathbf{k}_t^\textrm{T}\mathbf{k}_t},0\right\}\mathbf{k}_t$

### 改为

将前式代入后式可得$\lambda_t=\frac{\mathbf{k}_t^\mathrm{T}\mathbf{g}_t-\delta}{\mathbf{k}_t^\mathrm{T}\mathbf{k}_t}$。由于Lagrange乘子应当大等于0，所以Lagrange乘子应

为$\max\left\{\frac{{\mathbf{k}_t^\textrm{T}\mathbf{g}_t-\delta}}{\mathbf{k}_t^\textrm{T}\mathbf{k}_t},0\right\}$，优化问题的最优解为

$\mathbf{z}_t=\mathbf{g}_t-\max\left\{\frac{\mathbf{k}_t^\mathrm{T}\mathbf{g}_t-\delta}{\mathbf{k}_t^\textrm{T}\mathbf{k}_t},0\right\}\mathbf{k}_t$

## 第157页算法8-11第3.2.3步

$\mathbf{k}=\nabla_{\bftheta}{d_\textrm{KL}}\left(\pi\left(\cdot\mid\sfS_t;\bftheta^\mathrm{EMA}\right)||\pi\left(\cdot\mid\sfS_t;\bftheta\right)\right)$，$\mathbf{z}_t=\mathbf{g}_t-\max\left\{\frac{\mathbf{k}_t^{\rm{T}}\mathbf{g}-\delta}{\mathbf{k}_t^\rm{T}\mathbf{k}_t},0\right\}\mathbf{k}_t$

### 改为

$\mathbf{k}\leftarrow{\nabla_{\bftheta}}{d_{{\textrm{KL}}}}\left(\pi\left(\cdot|\sfS_t;\bftheta^\mathrm{EMA}\right)||\pi\left(\cdot\mid\sfS_t;\bftheta'\right)\right)$，$\mathbf{z}\leftarrow\mathbf{g}-\max\left\{\frac{\mathbf{k}^\textrm{T}\mathbf{g}-\delta}{\mathbf{k}^\textrm{T}\mathbf{k}},0\right\}\mathbf{k}$


## 第157页算法8-11

将第3.3步重新编号为第3.2.4步。

## 第158页第19行，第159页第1~2行

$\sfA_t=a$

### 改为

$\sfA_t=\sfa$

## 第159页第8行

和最优动作价值函数。

### 改为

和最优状态价值函数。

## 第159页第27行

其中目标$U^{\left(q\right)}_t=R_{t+1}+\gamma{v}\left(\sfS;\mathbf{w}^\left(v\right)_\text{目标}\right)$；

### 改为

其中目标$U^{\left(q\right)}_t=R_{t+1}+\gamma{v}\left(\sfS';\mathbf{w}^\left(v\right)_\text{目标}\right)$；

## 第160页算法8-12第2.2.2步

对应的回报$U^{\left(q\right)}_t=R_{t+1}+\gamma{v}\left(\sfS;\mathbf{w}^\left(v\right)_\text{目标}\right)$

### 改为

对应的回报$U^{\left(q\right)}_t=R_{t+1}+\gamma{v}\left(\sfS';\mathbf{w}^\left(v\right)_\text{目标}\right)$

### 第161页第4~5行

作者注：“动作价值网络和策略网络往往采用矢量形式的输出”指的是动作空间是离散动作空间的情况。如果动作空间是连续动作空间，往往用Gaussian形式的策略，详见GitHub代码。包括SAC算法在内的使用策略梯度的算法采用Gaussian形式策略后，策略被限制为了单峰（unimodal）形式。如果需要策略具有多峰（multimodal）形式，可以使用混合Gaussian模型（Gaussian Mixture Model，GMM）。

## 第161页8.6节

作者注：这个案例的动作空间是离散的。同时，本章介绍的算法，包括但不限于PPO、SAC等算法，也可用于动作空间是连续空间的情况。

## 第162页第7行

$D''=\frac{1}{2}\cos\Theta''+\frac{5}{4}$

### 改为

$D''_t=\frac{1}{2}\cos\Theta''_t+\frac{5}{4}$

## 第162页第11~12行

使得角速度有界$\dot\Theta_1\in\left[-4\pi,4\pi\right]$，$\dot\Theta_2\in\left[-9\pi,9\pi\right]$。

### 改为

使得角速度有界$\dot\Theta'_t\in\left[-4\pi,4\pi\right]$，$\dot\Theta''_t\in\left[-9\pi,9\pi\right]$。


## 第162页第17行

单5-7中的`qlearning()`函数

### 改为

单5-3中的`play_sarsa()`函数


## 第163页代码清单8-1

其中的`learn()`函数代码改为

```
def learn(self, observation, action, reward, next_observation, done,
        next_action=None):
    # 训练执行者网络
    x = observation[np.newaxis]
    u = self.critic_net.predict(x)
    q = u[0, action]
    x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)
    with tf.GradientTape() as tape:
        pi_tensor = self.actor_net(x_tensor)[0, action]
        logpi_tensor = tf.math.log(tf.clip_by_value(pi_tensor,
                1e-6, 1.))
        loss_tensor = -self.discount * q * logpi_tensor
    grad_tensors = tape.gradient(loss_tensor, self.actor_net.variables)
    self.actor_net.optimizer.apply_gradients(zip(
            grad_tensors, self.actor_net.variables))
    
    # 训练评论者网络
    u[0, action] = reward
    if not done:
        q = self.critic_net.predict(
                next_observation[np.newaxis])[0, next_action]
        u[0, action] += self.gamma * q
    self.critic_net.fit(x, u, verbose=0)
    
    if done:
        self.discount = 1.
    else:
        self.discount *= self.gamma
```

## 第164页第1行

中的`qlearning()`函数

### 改为

中的`play_qlearning()`函数

## 第164页代码清单8-2

其中的`learn()`函数代码改为

```
def learn(self, observation, action, reward, next_observation, done):
    x = observation[np.newaxis] # 特征
    u = reward + (1. - done) * self.gamma * self.critic_net.predict(
            next_observation[np.newaxis]) # 评论者目标
    td_error = u - self.critic_net.predict(x)
    
    # 训练执行者网络
    x_tensor = tf.convert_to_tensor(observation[np.newaxis],
            dtype=tf.float32)
    with tf.GradientTape() as tape:
        pi_tensor = self.actor_net(x_tensor)[0, action]
        logpi_tensor = tf.math.log(tf.clip_by_value(pi_tensor, 1e-6, 1.))
        loss_tensor = -self.discount * td_error * logpi_tensor
    grad_tensors = tape.gradient(loss_tensor, self.actor_net.variables)
    self.actor_net.optimizer.apply_gradients(zip(
            grad_tensors, self.actor_net.variables)) # 更新执行者网络
    
    # 训练评论者网络
    self.critic_net.fit(x, u, verbose=0) # 更新评论者网络
    
    if done:
        self.discount = 1. # 为下一回合初始化累计折扣
    else:
        self.discount *= self.gamma # 进一步累计折扣
```

## 第168页第3行

的`qlearning()`函数联合起来

### 改为

的`play_qlearning()`函数联合起来

## 第172页第17行

$q_{\pi\left(\bftheta\right)}\left(\sfs,\pi\left(\sfs;\bftheta\right)\right)=r\left(\sfs,\pi\left(\sfs;\bftheta\right)\right)+\gamma\sum\limits_{s'}{p\left(\sfs'|\sfs,\pi\left(\bftheta\right)\right){v_{\pi\left(\bftheta\right)}}\left(\sfs'\right)},\quad\sfs\in\mathcal{S}$

### 改为

$q_{\pi\left(\bftheta\right)}\left(\sfs,\pi\left(\sfs;\bftheta\right)\right)=r\left(\sfs,\pi\left(\sfs;\bftheta\right)\right)+\gamma\sum\limits_{\sfs'}{p\left(\sfs'|\sfs,\pi\left(\sfs;\bftheta\right)\right){v_{\pi\left(\bftheta\right)}}\left(\sfs'\right)},\quad\sfs\in\mathcal{S}$


## 第173页第2~4行、第6行、第10行（共5处）

$\sum\limits_{\sfs'}{p\left(\sfs'|\sfs;\pi\left(\bftheta\right)\right)\nabla{v_{\pi\left(\bftheta\right)}}\left(\sfs'\right)}$


改为

$\sum\limits_{\sfs'}{p\left(\sfs'|\sfs,\pi\left(\sfs;\bftheta\right)\right)\nabla{v_{\pi\left(\bftheta\right)}}\left(\sfs'\right)}$


## 第173页第9行

$\nabla{v}_{\pi\left(\bftheta\right)}\left(\sfS_t\right)$

### 改为

$\nabla{v}_{\pi\left(\bftheta\right)}\left(\sfs\right)$


## 第173页第20行

$=\E\left[\nabla\pi\left(\sfS_0;\bftheta\right){\left[\nabla_\sfa q_{\pi\left(\bftheta\right)}\left(\sfS_0,\sfa\right)\right]}_{\sfa=\pi\left(\sfS_0;\bftheta\right)}\right]+\gamma\E\left[\nabla\pi\left(\sfS_1;\bftheta\right){{\left[{\nabla_\sfa}{q_{\pi\left(\bftheta\right)}}\left(\sfS_1,\sfa\right)\right]}_{\sfa=\pi\left(\sfS_1;\bftheta\right)}}\right]+\gamma^2\E\left[\nabla{v}_{\pi\left(\bftheta\right)}\left(\sfS_1\right)\right]$

### 改为

$=\E\left[\nabla\pi\left(\sfS_0;\bftheta\right){\left[\nabla_\sfa q_{\pi\left(\bftheta\right)}\left(\sfS_0,\sfa\right)\right]}_{\sfa=\pi\left(\sfS_0;\bftheta\right)}\right]+\gamma\E\left[\nabla\pi\left(\sfS_1;\bftheta\right){{\left[{\nabla_\sfa}{q_{\pi\left(\bftheta\right)}}\left(\sfS_1,\sfa\right)\right]}_{\sfa=\pi\left(\sfS_1;\bftheta\right)}}\right]+\gamma^2\E\left[\nabla{v}_{\pi\left(\bftheta\right)}\left(\sfS_2\right)\right]$


## 第174页第4行

$\rho_\pi\left(\sfs\right)=\int\limits_{\sfs_0\in\mathcal{S}}p_{\sfS_0}\left(\sfs_0\right)\sum\limits_{t=0}^{+\infty}\gamma_t\Pr\left[\sfS_t|\sfS_0=\sfs_0;\bftheta\right]\mathrm{d}\sfs_0$

#### 改为

$\rho_{\pi\left(\bftheta\right)}\left(\sfs\right)=\sum\limits_{t=0}^{+\infty}\gamma_t\Pr\left[\sfS_t=\sfs;\pi\left(\bftheta\right)\right]$, $\sfs\in\mathcal{S}$

作者注：从严格意义上说，有折扣的状态分布并不是概率分布，因为它的和不总是1。针对有折扣的状态分布的期望也只是采用了期望的形式。


## 第174页第7~11行

$=\sum\limits_{t=0}^{+\infty}\int\limits_{\sfs}p_{\sfS_t}\left(\sfs\right)\gamma^t\nabla\pi\left(\sfs;\bftheta\right)\left[\nabla_\sfs{q}_{\pi\left(\bftheta\right)}\left(\sfs,\sfa\right)\right]_{\sfa=\pi\left(\sfS_t;\bftheta\right)}\mathrm{d}\sfs$

$=\sum\limits_{t=0}^{+\infty}\int\limits_{\sfs}\left(\int_{\sfs_0}\int\limits_{\sfs_0}p_{\sfS_0}\left(\sfs_0\right)\Pr\left[\sfS_t=\sfs|\sfs_0=\sfs_0;\bftheta\right]\mathrm{d}\sfs_0\right)\gamma^t\nabla\pi\left(\sfs;\bftheta\right)\left[\nabla_\sfs{q}_{\pi\left(\bftheta\right)}\left(\sfs,\sfa\right)\right]_{\sfa=\pi\left(\sfs;\bftheta\right)}\mathrm{d}\sfs$

$=\int\limits_{\sfs}\left(\int\limits_{\sfs_0}p_{\sfS_0}\left(\sfs_0\right)\sum\limits_{t=0}^{+\infty}\gamma^t\Pr\left[\sfS_t=\sfs|\sfs_0=\sfs_0;\bftheta\right]\mathrm{d}\sfs_0\right)\nabla\pi\left(\sfS_t;\bftheta\right)\left[\nabla_\sfa{q}_{\pi\left(\bftheta\right)}\right]_{\sfa=\pi\left(\sfs;\bftheta\right)}\mathrm{d}\sfs$

$=\int\limits_{\sfs}\rho_{\pi\left(\bftheta\right)}\left(\sfs\right)\nabla\pi\left(\sfs;\bftheta\right)\left[\nabla_\sfa{q}_{\pi\left(\bftheta\right)}\left(\sfs,\sfa\right)\right]_{\sfa=\pi\left(\sfs;\bftheta\right)}\mathrm{d}\sfs$

$=\E_{\rho_{\pi\left(\bftheta\right)}}\left[\nabla\pi\left(\sfs;\bftheta\right)\left[\nabla_\sfa{q}_{\pi\left(\bftheta\right)}\left(\sfs,\sfa\right)\right]_{\sfa=\pi\left(\sfs;\bftheta\right)}\right]$

#### 改为

$=\sum\limits_{t=0}^{+\infty}\sum\limits_{\sfs}^{}\Pr\left[\sfS_t=\sfs;\pi\left(\bftheta\right)\right]\gamma^t\nabla\pi\left(\sfs;\bftheta\right)\left[\nabla_\sfa{q}_{\pi\left(\bftheta\right)}\left(\sfs,\sfa\right)\right]_{\sfa=\pi\left(\sfs;\bftheta\right)}$

$=\sum\limits_{\sfs}^{}\left(\sum\limits_{t=0}^{+\infty}\gamma^t\Pr\left[\sfS_t=\sfs;\pi\left(\bftheta\right)\right]\right)\nabla\pi\left(\sfs;\bftheta\right)\left[\nabla_\sfa{q}_{\pi\left(\bftheta\right)}\left(\sfs,\sfa\right)\right]_{\sfa=\pi\left(\sfs;\bftheta\right)}$

$=\sum\limits_{\sfs}^{}\rho_{\pi\left(\bftheta\right)}\left(\sfs\right)\nabla\pi\left(\sfs;\bftheta\right)\left[\nabla_\sfa{q}_{\pi\left(\bftheta\right)}\right]_{\sfa=\pi\left(\sfs;\bftheta\right)}$

$=\E_{\rho_{\pi\left(\bftheta\right)}}\left[\nabla\pi\left(\sfS;\bftheta\right)\left[\nabla_\sfa{q}_{\pi\left(\bftheta\right)}\left(\sfS,\sfa\right)\right]_{\sfa=\pi\left(\sfS;\bftheta\right)}\right]$


## 第175页第7行

其中$\sfA_\min$和$\sfA_\max$是动作的最小取值和最大取值

### 改为

其中$\sfA_\text{low}$和$\sfA_\text{high}$是动作的最小取值和最大取值

## 第175页算法9-1第2.2步

2.2（决定初始动作）

### 改为

2.2（初始化状态动作对）选择状态$\sfS$，

## 第176页第8行

方差为$\frac{\sigma^2}{2\theta}\left(1-e^{-2\theta}\right)$，

### 改为

方差为$\frac{\sigma^2}{2\theta}\left(1-e^{-2\theta{t}}\right)$，


## 第176页第12行

$\sigma^2e^{-\theta\left(t+s\right)}\E\left[\int_{0}^{t}e^{\theta\tau}\mathrm{d}B_t\int_{0}^{s}e^{\theta\tau}\mathrm{d}B_t\right]$

#### 改为

$\sigma^2e^{-\theta\left(t+s\right)}\E\left[\int_{0}^{t}e^{\theta\tau}\mathrm{d}B_\tau\int_{0}^{s}e^{\theta\tau}\mathrm{d}B_\tau\right]$


## 第176页到第177页9.2节正文

### 改为

对于连续的动作空间，我们希望能够找到一个确定性策略，使得每条轨迹的回报最大。同策确定性算法利用策略$\pi\left(\bftheta\right)$生成轨迹，并在这些轨迹上求得回报的平均值，通过让平均回报最大，使得每条轨迹上的回报尽可能大。事实上，如果每条轨迹的回报都要最大，那么对于任意策略$b$采样得到的轨迹，我们都希望在这套轨迹上的平均回报最大。所以异策确定性策略算法引入确定性行为策略$b$，将这个平均改为针对策略$b$采样得到的轨迹，得到异策确定性梯度为$\nabla\E_{\rho_b}\left[q_{\pi\left(\bftheta\right)}\left(\sfS,\pi\left(\sfS;\bftheta\right)\right)\right]=\E_{\rho_b}\left[\nabla\pi\left(\sfS;\bftheta\right)\left[\nabla_a{q}_{\pi\left(\bftheta\right)}\left(\sfS,\sfa\right)\right]_{\sfa=\pi\left(\sfS;\bftheta\right)}\right]$。这个表达式与同策的情形相比，期望运算针对的表达式相同。所以，异策确定性算法的迭代式与同策确定性算法的迭代式相同。

异策确定性算法可能比同策确定性算法性能好的原因在于，行为策略可能会促进探索，用行为策略采样得到的轨迹能够更加全面的探索轨迹空间。这时候，最大化对轨迹分布的平均期望时能够同时考虑到更多不同的轨迹，使得在整个轨迹空间上的所有轨迹的回报会更大。

与非确定性策略梯度相比，非确定性异策算法的迭代式中含有重采样因子$\frac{\pi\left(\sfA_t|\sfS_t;\bftheta\right)}{b\left(\sfA_t|\sfS_t\right)}$，而确定性异策算法中没有。这是因为，确定性的行为策略$b$并不对于确定性的目标策略$\pi\left(\bftheta\right)$绝对连续，重采样因子没有定义，所以不包括重采样因子。


## 第177页算法9-2第2.1步和2.2步之间

### 增加

2.2（初始化状态）选择状态$\sfS$；

2.3（确定行为策略）确定行为策略$b$；

## 第177页算法9-2第2.2.1步

2.2.1（执行）用$b(\cdot\mid\sfS)$得到

### 改为

2.4.1（执行）用$b(\sfS)$得到

## 第178页算法9-3第2.2.2步

$U\leftarrow{R}+\gamma{q}\left(\sfS',\pi\left(\sfS';\bftheta\right)\right)$

### 改为

$U\leftarrow{R}+\gamma{q}\left(\sfS',\pi\left(\sfS';\bftheta_\text{目标}\right);\mathbf{w}_\text{目标}\right)$


## 第179页算法9-4第2.2.3步

$U=R+\gamma\min_{i=0,1}q\left(\sfS',\pi\left(\sfS';\bftheta_\text{目标}\right);\mathbf{w}^{\left(i\right)}\right)$

### 改为

$U\leftarrow{R}+\gamma\min_{i=0,1}q\left(\sfS',\sfA';\mathbf{w}^{\left(i\right)}_\text{目标}\right)$

## 第180页9.3节

作者注：Gym 0.21版本将Pendulum-v0改为Pendulum-v1。

## 第180页图9-1

### 改为

<img src="./images/figure09_01.png" style="width: 200px;"/>


## 第180页第4行

$X$轴是水平向下的，$Y$轴是水平向右的。

### 改为

$X$轴是水平向上的，$Y$轴是水平向左的。


## 第180页第8行

下一观测$\left(\cos\Theta_{t+1},\sin\Theta_{t+1},\dot\Theta\right)$。

### 改为

下一观测$\left(\cos\Theta_{t+1},\sin\Theta_{t+1},\dot\Theta_{t+1}\right)$。


## 第180页表9-1最右边一列

| 倒立摆（Pendulum-v0）                                        |
| ------------------------------------------------------------ |
| $\left[-\pi,\pi\right)\times\left[-4\pi,4\pi\right]\times\left[-9\pi,9\pi\right]$ |
| ${\left[-1,1\right]}^2\times\left[-4\pi,4\pi\right]\times\left[-9\pi,9\pi\right]$ |
| $\left[-2,2\right]$                                          |
| $\left[-\pi^2-6.404,0\right]$                                |


### 改为

| 倒立摆（Pendulum-v0）                          |
| ---------------------------------------------- |
| $\left[-\pi,\pi\right)\times\left[-8,8\right]$ |
| $\left[-1,1\right]^2\times\left[-8,8\right]$   |
| $\left[-2,2\right]$                            |
| $\left[-\pi^2-6.404,0\right]$                  |


## 第180页第18-19行

$\Theta_{t+1}\leftarrow\Theta_t+0.05\left(\dot\Theta-0.75\sin\Theta-0.15\sfA_t\right)$在$\left[-\pi,+\pi\right)$的主值区间

$\dot\Theta_{t+1}\leftarrow\text{clip}\left(\dot\Theta_t-0.75\sin\Theta_t-0.15\sfA_t,-8,+8\right)$

### 改为

$\Theta_{t+1}\leftarrow\Theta_t+0.05\left(\dot\Theta_t+0.75\sin\Theta_t+0.15\sfA_t\right)$在$\left[-\pi,+\pi\right)$的主值区间

$\dot\Theta_{t+1}\leftarrow\text{clip}\left(\dot\Theta_t+0.75\sin\Theta_t+0.15\sfA_t,-8,+8\right)$


## 第181页第1行

值$\left|\dot{\Theta}\right|$和

### 改为

值$\left|\dot{\Theta}_t\right|$和


## 第189页

`gym[atari]`已官方支持Windows。Gym <=0.20：在Windows、macOS、Linux系统下均只需用下列命令就可以完成`gym[atari]`的安装：Gym 0.21又更改了安装方式，需要再下载ROM。

```
pip install --upgrade gym[atari]
```

## 第191页表10-1（共4处）

NoFrameSkip

### 改为

NoFrameskip

## 第204页第5~6行

极大极小算法（monimax）

### 改为

极大极小算法（minimax）


## 第207页图11-3

### 改为

<img src="./images/figure11_03.png" style="width: 400px;"/>