# 《强化学习：原理与Python实现》更新与勘误

（2020年11月第1版第5次印刷）

Gym 0.25版本更新了API，GitHub代码已相应更新。代码以GitHub上为准。

### 行数计算方法

本勘误文档中，行数计算“第$i$行”（$i=0,1,2,\ldots$）是从0开始计数的。小节标题、公式、内联代码、注意、本章要点记入行数，章标题、图、表、代码清单及它们的题注不计入行数。空行不计入行数。

$
\newcommand{\sfA}{\unicode{x1d608}}
\newcommand{\sfS}{\unicode{x1d61a}}
\newcommand{\sfa}{\unicode{x1d622}}
\newcommand{\sfs}{\unicode{x1d634}}
\newcommand{\bftheta}{\pmb{\unicode{x3B8}}}
\newcommand{\E}{\textrm{E}}
$


## 第17页第6行

作者注：
“轨道”又称为“轨迹”。本书中这两个词混用。

## 第22页图2-2b)中图注

$v_\pi(\sfs)$

#### 改为

$v_\pi(\sfs')$

## 第28页第1行

分母部分

### 改为

分子部分

## 第29页正文第2行

$\gamma<\frac{1-4\beta}{1-\alpha}$

#### 改为

$\gamma\ge\frac{1-4\beta}{1-\alpha}$

## 第30页

作者注：用线性规划求解最优状态价值的详细证明可见《Markov Decision Processes: Discrete Stochastic Dynamic Programming》（Martin Puterman）的第6.9节。证明大致如下：当$c\left(\sfs\right)$为Markov决策过程的初始状态分布时，原问题和对偶问题的目标都是Markov决策过程的平均回合奖励，原问题的最优解是最优状态价值。对$c\left(\sfs\right)$做灵敏度分析可知，无论$c\left(\sfs\right)$取什么分布，对偶问题均有解（这利用到对偶问题的可行域就是带折扣的状态动作对组成的分布所在的空间），所以原问题的最优解都是不变的。也就是说，Markov决策过程的最优状态价值和初始状态分布无关。进一步，可以在原问题中放宽$\sum\nolimits_{\sfs}c\left(\sfs\right)=1$这个限制，原问题的解依然不变，只是优化目标进行了放缩。


## 第34页第9行

计算这个动态规划问题

### 改为

计算这个线性规划问题

## 第39页第1行

$v_{*}\left(\sfs'\right)$

### 改为

$v\left(\sfs'\right)$

## 第39页第25行

（fix point）

### 改为

（fixed point）

## 第48页3.5节

作者注：Gym 0.19版本将FrozenLake-v0改为FrozenLake-v1。

作者注：Gym 0.22删除了DiscreteEnv类，所以env.unwrapped.nS和env.unwrapped.nA不再可用。建议用env.observation_space.n替换env.unwrapped.nA，用env.action_space.n替换env.unwrapped.nA。

## 第58页最后一行

态$\sfs_{起始}$

#### 改为

态$\sfs_{开始}$

## 第66页4.3节

作者注：Gym 0.20版本将Blackjack-v0改为Blackjack-v1。

## 第67页第3行

范围为3~21的int型数值

### 改为

范围为4~21的int型数值


## 第73页正文第0行

`evaluate_action_monte_carlo_importance_resample`

### 改为

`evaluate_monte_carlo_importance_sample`


## 第74页正文第0行，第74页正文第2行

`monte_carlo_importance_resample`

改为

`monte_carlo_importance_sample`

## 第85页算法5-10输入

输入：环境（无数学描述）、策略$\pi$。

#### 改为

输入：环境（无数学描述）、如果是策略评估还需要输入策略$\pi$。

## 第85页算法5-10第2.3.1步

根据$\pi\left(\cdot\mid\sfS_{t+n}\right)$决定

#### 改为

根据$b\left(\cdot\mid\sfS_{t+n}\right)$决定

## 第91页算法5-13参数部分、第92页算法5-14参数部分

参数：资格迹参数$\beta$、

#### 改为

参数：资格迹参数$\lambda$和$\beta$、

## 第91页算法5-13第2.3.6步

退出2.2步

#### 改为

退出2.3步


## 第102页第12行

$\sum\nolimits_{t=0}^T{{\left[G_t-v\left(\sfS_t;\mathbf{w}\right)\right]}^2}$

### 改为

$\sum\nolimits_{t=0}^{T-1}{{\left[G_t-v\left(\sfS_t;\mathbf{w}\right)\right]}^2}$


## 第104页算法6-4第2.2.4步，第106页算法6-6第2.3.5步（共2处）

（更新动作价值函数）

### 改为

（更新价值函数）

## 第105页算法6-5第2.1步

#### 增加

初始化资格迹$\mathbf{z}\leftarrow\mathbf{0}$

## 第114页第5行

算法6-10

### 改为

算法6-9

## 第114页6.4.4节

本节将“duel network”译作“对偶网络”有误。可译为“决斗网络”。相应的“对偶Q网络”、“对偶深度Q网络”可改为“决斗Q网络”、“决斗深度Q网络”。


## 第118页

作者注：

砖瓦编码是一种历史悠久的特征构造方法，可用于回归、分类等问题。目前学术界倾向于用神经网络代替砖瓦编码来构造特征。由于砖瓦编码和强化学习没有直接关联，本书没有用过多的篇幅介绍砖瓦编码。

实际使用砖瓦编码时，不需要精确计算砖瓦的数量，常随意的大致估计砖瓦的数量作为特征数。如果设置的特征数大于真实的砖瓦数量，那么有些特征永远不会取到，有些浪费；如果设置的特征数小于真实的砖瓦数量，那么有多个砖瓦需要共享特征，具体逻辑可以见代码清单6-3中“冲突处理”部分。这些浪费和冲突往往不会造成明显的性能损失。

第118页砖瓦数计算：每个大网格的网格宽度刚好是整个取值范围的1/8。所以，第0层大网格每个维度有8个大网格；第1~7层由于有偏移，每个维度需要有9个大网格才能覆盖整个取值范围。第117页图6-3b的情况略有不同：这个图中每个维度的取值范围不是大网格的长度的整数倍。所以有些层偏移后，不需要更多的大网格也可以覆盖整个取值范围。

## 第127页第10行

$=\E\left[\sum\limits_\sfa{q}_{\pi\left(\bftheta\right)}\left(\sfS_0,\sfa\right)\nabla\pi\left(\sfa|\sfS_0;\bftheta\right)\right]+\E\left[\sum\limits_\sfa\gamma{q}_{\pi\left(\bftheta\right)}\left(\sfS_1,\sfa\right)\nabla\pi\left(\sfa|\sfS_1;\bftheta\right)\right]+\gamma^2\E\left[\nabla{v}_{\pi\left(\bftheta\right)}\left(\sfS_1\right)\right]$

改为

$=\E\left[\sum\limits_\sfa{q}_{\pi\left(\bftheta\right)}\left(\sfS_0,\sfa\right)\nabla\pi\left(\sfa|\sfS_0;\bftheta\right)\right]+\E\left[\sum\limits_\sfa\gamma{q}_{\pi\left(\bftheta\right)}\left(\sfS_1,\sfa\right)\nabla\pi\left(\sfa|\sfS_1;\bftheta\right)\right]+\gamma^2\E\left[\nabla{v}_{\pi\left(\bftheta\right)}\left(\sfS_2\right)\right]$


## 第128页第10行

增量$\alpha\gamma^tG_t\nabla\pi\left(\sfA_t|\sfS_t;\bftheta_t\right)$

#### 改为

增量$\alpha\gamma^tG_t\nabla\pi\left(\sfA_t|\sfS_t;\bftheta\right)$

## 第131页算法7-3第2.3步

2.3（初始化回报和权重）

#### 改为

2.3（初始化回报）


## 第137页代码清单7-1中`learn()`函数

```
            y = np.eye(self.action_n)[df['action']] * \
                    df['psi'].values[:, np.newaxis]
            self.policy_net.fit(x, y, verbose=0)
```

#### 改为

```
            sample_weight = df['psi'].values[:, np.newaxis]
            y = np.eye(self.action_n)[df['action']]
            self.policy_net.fit(x, y, sample_weight=sample_weight, verbose=0)
```

## 第142页算法8.3第2.3.2步

作者注：这里的更新式子遵循了论文原文而没有考虑累积折扣。推导出现折扣是正确的；更新时考虑折扣也是正确和合理的。

## 第143页第5行，第143页算法8.4第2.3.4步（共2处）

$\nabla\ln\pi\left(\sfA\mid\sfS;\mathbf{w}\right)$

改为

$\nabla\ln\pi\left(\sfA\mid\sfS;\bftheta\right)$

## 第144页第15行和第18行（共2处）

$\E_{\sfS_t,\sfA_t\sim\pi\left(\bftheta\right)}\left[a_{\pi_k}\left(\sfS_t,\sfA_t\right)\right]$

#### 改为

$\E_{\sfS_t,\sfA_t\sim\pi\left(\bftheta\right)}\left[a_{\pi_{\left(\bftheta_k\right)}}\left(\sfS_t,\sfA_t\right)\right]$


## 第147页图8-1中间线的注记

$l_{\left(\bftheta\right)}$

### 改为

$l\left(\bftheta\right)$


## 第150页算法8-7第2.1步

2.1  $\mathbf{z}\leftarrow\mathbf{Fx}$

### 改为

2.1  $\mathbf{z}\leftarrow\mathbf{Fp}$

### 第161页第4~5行

作者注：“动作价值网络和策略网络往往采用矢量形式的输出”指的是动作空间是离散动作空间的情况。如果动作空间是连续动作空间，往往用Gaussian形式的策略，详见GitHub代码。包括SAC算法在内的使用策略梯度的算法采用Gaussian形式策略后，策略被限制为了单峰（unimodal）形式。如果需要策略具有多峰（multimodal）形式，可以使用混合Gaussian模型（Gaussian Mixture Model，GMM）。

## 第161页8.6节

作者注：这个案例的动作空间是离散的。同时，本章介绍的算法，包括但不限于PPO、SAC等算法，也可用于动作空间是连续空间的情况。


## 第162页第17行

单5-7中的

### 改为

单5-3中的


## 第165页代码清单8-2前4行（中间有个空行不计入）

多缩进4个空格


## 第173页第9行

$\nabla{v}_{\pi\left(\bftheta\right)}\left(\sfS_t\right)$

### 改为

$\nabla{v}_{\pi\left(\bftheta\right)}\left(\sfs\right)$


## 第174页第4行

$\rho_\pi\left(\sfs\right)=\int\limits_{\sfs_0\in\mathcal{S}}p_{\sfS_0}\left(\sfs_0\right)\sum\limits_{t=0}^{+\infty}\gamma_t\Pr\left[\sfS_t|\sfS_0=\sfs_0;\bftheta\right]\mathrm{d}\sfs_0$

#### 改为

$\rho_{\pi\left(\bftheta\right)}\left(\sfs\right)=\sum\limits_{t=0}^{+\infty}\gamma_t\Pr\left[\sfS_t=\sfs;\pi\left(\bftheta\right)\right]$, $\sfs\in\mathcal{S}$

作者注：从严格意义上说，有折扣的状态分布并不是概率分布，因为它的和不总是1。针对有折扣的状态分布的期望也只是采用了期望的形式。


## 第174页第7~11行

$=\sum\limits_{t=0}^{+\infty}\int\limits_{\sfs}p_{\sfS_t}\left(\sfs\right)\gamma^t\nabla\pi\left(\sfs;\bftheta\right)\left[\nabla_\sfs{q}_{\pi\left(\bftheta\right)}\left(\sfs,\sfa\right)\right]_{\sfa=\pi\left(\sfS_t;\bftheta\right)}\mathrm{d}\sfs$

$=\sum\limits_{t=0}^{+\infty}\int\limits_{\sfs}\left(\int_{\sfs_0}\int\limits_{\sfs_0}p_{\sfS_0}\left(\sfs_0\right)\Pr\left[\sfS_t=\sfs|\sfs_0=\sfs_0;\bftheta\right]\mathrm{d}\sfs_0\right)\gamma^t\nabla\pi\left(\sfs;\bftheta\right)\left[\nabla_\sfs{q}_{\pi\left(\bftheta\right)}\left(\sfs,\sfa\right)\right]_{\sfa=\pi\left(\sfs;\bftheta\right)}\mathrm{d}\sfs$

$=\int\limits_{\sfs}\left(\int\limits_{\sfs_0}p_{\sfS_0}\left(\sfs_0\right)\sum\limits_{t=0}^{+\infty}\gamma^t\Pr\left[\sfS_t=\sfs|\sfs_0=\sfs_0;\bftheta\right]\mathrm{d}\sfs_0\right)\nabla\pi\left(\sfS_t;\bftheta\right)\left[\nabla_\sfa{q}_{\pi\left(\bftheta\right)}\right]_{\sfa=\pi\left(\sfs;\bftheta\right)}\mathrm{d}\sfs$

$=\int\limits_{\sfs}\rho_{\pi\left(\bftheta\right)}\left(\sfs\right)\nabla\pi\left(\sfs;\bftheta\right)\left[\nabla_\sfa{q}_{\pi\left(\bftheta\right)}\left(\sfs,\sfa\right)\right]_{\sfa=\pi\left(\sfs;\bftheta\right)}\mathrm{d}\sfs$

$=\E_{\rho_{\pi\left(\bftheta\right)}}\left[\nabla\pi\left(\sfs;\bftheta\right)\left[\nabla_\sfa{q}_{\pi\left(\bftheta\right)}\left(\sfs,\sfa\right)\right]_{\sfa=\pi\left(\sfs;\bftheta\right)}\right]$

#### 改为

$=\sum\limits_{t=0}^{+\infty}\sum\limits_{\sfs}^{}\Pr\left[\sfS_t=\sfs;\pi\left(\bftheta\right)\right]\gamma^t\nabla\pi\left(\sfs;\bftheta\right)\left[\nabla_\sfa{q}_{\pi\left(\bftheta\right)}\left(\sfs,\sfa\right)\right]_{\sfa=\pi\left(\sfs;\bftheta\right)}$

$=\sum\limits_{\sfs}^{}\left(\sum\limits_{t=0}^{+\infty}\gamma^t\Pr\left[\sfS_t=\sfs;\pi\left(\bftheta\right)\right]\right)\nabla\pi\left(\sfs;\bftheta\right)\left[\nabla_\sfa{q}_{\pi\left(\bftheta\right)}\left(\sfs,\sfa\right)\right]_{\sfa=\pi\left(\sfs;\bftheta\right)}$

$=\sum\limits_{\sfs}^{}\rho_{\pi\left(\bftheta\right)}\left(\sfs\right)\nabla\pi\left(\sfs;\bftheta\right)\left[\nabla_\sfa{q}_{\pi\left(\bftheta\right)}\right]_{\sfa=\pi\left(\sfs;\bftheta\right)}$

$=\E_{\rho_{\pi\left(\bftheta\right)}}\left[\nabla\pi\left(\sfS;\bftheta\right)\left[\nabla_\sfa{q}_{\pi\left(\bftheta\right)}\left(\sfS,\sfa\right)\right]_{\sfa=\pi\left(\sfS;\bftheta\right)}\right]$


## 第176页第12行

$\sigma^2e^{-\theta\left(t+s\right)}\E\left[\int_{0}^{t}e^{\theta\tau}\mathrm{d}B_t\int_{0}^{s}e^{\theta\tau}\mathrm{d}B_t\right]$

#### 改为

$\sigma^2e^{-\theta\left(t+s\right)}\E\left[\int_{0}^{t}e^{\theta\tau}\mathrm{d}B_\tau\int_{0}^{s}e^{\theta\tau}\mathrm{d}B_\tau\right]$

## 第177页算法9-2第2.3.1步

2.3.1（执行）用$b(\cdot\mid\sfS)$得到

### 改为

2.3.1（执行）用行为策略$b(\sfS)$得到

## 第180页9.3节

作者注：Gym 0.21版本将Pendulum-v0改为Pendulum-v1。

## 第188页10.1节

作者注：Gym 0.21版本修改了Atari游戏环境的安装方式。


## 第191页表10-1（共4处）

NoFrameSkip

### 改为

NoFrameskip

